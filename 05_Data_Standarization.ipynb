{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7957d1-61f4-4630-bceb-e7ff9279ae0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Data Preparation in ML - Notebook 05\n",
    "## Data Standardization Fundamentals\n",
    "\n",
    "**Part of the Databricks Data Preparation in ML Training Series**\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook covers essential data standardization techniques required for Databricks ML Associate Certification:\n",
    "\n",
    "- **Feature Scaling** - StandardScaler, MinMaxScaler, RobustScaler methods\n",
    "- **Normalization** - Unit vectors and L2 normalization techniques\n",
    "- **String Standardization** - Case, whitespace, and format cleaning\n",
    "- **Date/Time Standardization** - Consistent formats and timezone handling\n",
    "- **Categorical Standardization** - Uniform category naming conventions\n",
    "- **Outlier Treatment** - Detection and handling strategies for extreme values\n",
    "\n",
    "## Duration: ~40 minutes\n",
    "## Level: Fundamental → Intermediate\n",
    "\n",
    "---\n",
    "\n",
    "## Why is Data Standardization Critical?\n",
    "\n",
    "**Data Standardization** is a crucial preprocessing step for ML model success:\n",
    "- **Algorithm Performance** - Many algorithms require features on similar scales\n",
    "- **Convergence Speed** - Gradient descent converges faster with standardized features\n",
    "- **Feature Importance** - Ensures equal treatment regardless of original scale\n",
    "- **Distance-based Algorithms** - KNN, clustering, and SVM require standardized inputs\n",
    "\n",
    "---\n",
    "\n",
    "## Theory: Standardization Types\n",
    "\n",
    "### Feature Scaling Methods\n",
    "\n",
    "Different scaling methods serve different purposes and algorithm requirements:\n",
    "\n",
    "```\n",
    "Standard Scaling (Z-score): (x - μ) / σ\n",
    "- Centers data around 0 with unit variance\n",
    "- Best for: Linear algorithms, neural networks\n",
    "\n",
    "Min-Max Scaling: (x - min) / (max - min)  \n",
    "- Scales data to [0,1] range\n",
    "- Best for: When bounded range is required\n",
    "\n",
    "Robust Scaling: (x - median) / IQR\n",
    "- Uses median and interquartile range\n",
    "- Best for: Data with outliers\n",
    "```\n",
    "\n",
    "### Normalization Methods\n",
    "\n",
    "Normalization adjusts individual samples rather than features:\n",
    "\n",
    "```\n",
    "L2 Normalization: x / ||x||₂\n",
    "- Unit length vectors\n",
    "- Best for: Text analysis, cosine similarity\n",
    "\n",
    "L1 Normalization: x / ||x||₁  \n",
    "- Manhattan distance normalization\n",
    "- Best for: Sparse data scenarios\n",
    "\n",
    "Unit Vector: x / |x|\n",
    "- Simple magnitude normalization\n",
    "- Best for: Direction-based analysis\n",
    "```\n",
    "\n",
    "### Selection Guidelines:\n",
    "- **Linear Models** → Standard Scaling\n",
    "- **Tree-based Models** → Often no scaling needed\n",
    "- **Neural Networks** → Standard or Min-Max Scaling\n",
    "- **Distance-based** → Standard or Robust Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "718aaba2-5445-4507-83f7-dfb98adfee21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fb709b3-6bd3-4741-8e39-b2e02fa72452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Basic imports for Databricks ML\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, trim, lower, upper, regexp_replace, \n",
    "    to_timestamp, date_format, percentile_approx,\n",
    "    mean, stddev, min as spark_min, max as spark_max\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.ml.feature import StandardScaler, MinMaxScaler, RobustScaler, Normalizer, VectorAssembler\n",
    "from pyspark.ml.stat import Summarizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "064efa33-f20d-48e2-aa83-2fcf3a5a3b09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a demonstration dataset with different scales\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 1000\n",
    "ages = np.random.randint(12, 35, n_samples).clip(18, 65).tolist()\n",
    "salaries = np.random.lognormal(10.5, 0.8, n_samples).tolist()  # Different scales!\n",
    "heights = np.random.normal(170, 10, n_samples).clip(150, 200).tolist()\n",
    "scores = np.random.uniform(0, 100, n_samples).tolist()\n",
    "years_exp = np.random.exponential(5, n_samples).clip(0, 30).tolist()\n",
    "\n",
    "# Adding outliers\n",
    "outlier_indices = np.random.choice(n_samples, 20, replace=False)\n",
    "for idx in outlier_indices:\n",
    "    salaries[idx] = salaries[idx] * 3  # Extreme outliers\n",
    "\n",
    "# Different string formats for standardization\n",
    "cities = [\"  warsaw  \", \"KRAKÓW\", \"gdańsk\", \"Poznań \", \"  WROCŁAW\"]\n",
    "departments = [\"  IT\", \"Finance \", \"MARKETING\", \"hr  \", \"Sales\"]\n",
    "city_samples = np.random.choice(cities, n_samples).tolist()\n",
    "dept_samples = np.random.choice(departments, n_samples).tolist()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"height\", DoubleType(), True),\n",
    "    StructField(\"score\", DoubleType(), True),\n",
    "    StructField(\"years_experience\", DoubleType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True)\n",
    "])\n",
    "\n",
    "data = [(i, int(ages[i]), float(salaries[i]), float(heights[i]), \n",
    "         float(scores[i]), float(years_exp[i]), city_samples[i], dept_samples[i]) \n",
    "        for i in range(n_samples)]\n",
    "\n",
    "df_raw = spark.createDataFrame(data, schema)\n",
    "display(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7aef77f-276a-4584-9d21-01a8f8b423c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Feature Scaling - Numerical Features\n",
    "\n",
    "## Teoria\n",
    "**Feature Scaling** sprowadza wszystkie numerical features do podobnej skali, co jest krytyczne dla wielu algorytmów ML.\n",
    "\n",
    "### Kiedy używać różnych scalers:\n",
    "\n",
    "| Scaler | Kiedy używać | Właściwości |\n",
    "|--------|--------------|-------------|\n",
    "| **StandardScaler** | Normalne rozkłady | μ=0, σ=1 |\n",
    "| **MinMaxScaler** | Uniformne rozkłady | [0,1] range |\n",
    "| **RobustScaler** | Outliers present | Median-based |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8afd2151-334f-4806-812e-18ed836154b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scale analysis before standardization\n",
    "numerical_cols = [\"age\", \"salary\", \"height\", \"score\", \"years_experience\"]\n",
    "\n",
    "print(\"Scale analysis of numerical variables:\")\n",
    "for col_name in numerical_cols:\n",
    "    stats = df_raw.select(\n",
    "        spark_min(col_name).alias(\"min\"),\n",
    "        spark_max(col_name).alias(\"max\"),\n",
    "        mean(col_name).alias(\"mean\"),\n",
    "        stddev(col_name).alias(\"std\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"  {col_name:15} | Min: {stats['min']:8.1f} | Max: {stats['max']:10.1f} | Mean: {stats['mean']:8.1f} | Std: {stats['std']:8.1f}\")\n",
    "\n",
    "print(\"\\n  Problem: Salary has a much larger scale than other variables!\")\n",
    "print(\"    This can dominate ML algorithms based on distance (KNN, SVM, Neural Networks)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7696472e-cb10-4435-be13-d1253fca8ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_raw.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12024f5f-b1c7-4692-9e9e-b2b5d253af3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numerical_cols = [\"age\", \"salary\", \"height\", \"score\", \"years_experience\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d636d4cb-0f15-4c09-a29a-dc7dd4ab3bb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data preparation for scaling\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numerical_cols,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "df_assembled = assembler.transform(df_raw)\n",
    "display(df_assembled.select(\"features_raw\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac1223f5-ba87-4ba2-b91c-44d72f90b970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## StandardScaler - Z-score Normalization\n",
    "\n",
    "**Formula**: `(x - μ) / σ`\n",
    "\n",
    "✅ **Używaj gdy**: Features mają normalny rozkład"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccd82ca0-8aef-4222-b7f8-ea16b1fb3e1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# StandardScaler - most commonly used\n",
    "standard_scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features_standard\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "standard_model = standard_scaler.fit(df_assembled)\n",
    "df_standard = standard_model.transform(df_assembled)\n",
    "\n",
    "display(df_standard.select(\"features_standard\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee3a9536-1b05-4508-aef0-93695c6dec93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After applying feature standardization (e.g., using StandardScaler), it is important to verify whether the transformation was successful. The two key statistical indicators we typically check are:\n",
    "\t•\tMean ≈ 0\n",
    "\t•\tStandard deviation ≈ 1\n",
    "\n",
    "We use pyspark.ml.stat.Summarizer to compute these values for each feature in the assembled feature vector.\n",
    "\n",
    "The method works as follows:\n",
    "We define the summarization metrics (mean and std), then apply .summary() on the vectorized feature column, and finally select the result with an alias.\n",
    "\n",
    "If the standardization was applied correctly, the output should show that:\n",
    "\n",
    "\t•\tEach feature has a mean close to zero,\n",
    "  \n",
    "\t•\tEach feature has a standard deviation close to one.\n",
    "\n",
    "This is a crucial preprocessing step for many machine learning algorithms such as linear regression, logistic regression, or clustering, which assume features are on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb8249c5-a72b-4cf1-9d75-8e1cd033461c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753556840018}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weryfikacja: mean ≈ 0, std ≈ 1\n",
    "summary = Summarizer.metrics(\"mean\", \"std\").summary(df_standard[\"features_standard\"])\n",
    "display(df_standard.select(summary.alias(\"summary\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6e16ca4-d530-4b73-aa5a-8718422bf02c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MinMaxScaler - Range Normalization\n",
    "\n",
    "**Formula**: `(x - min) / (max - min)`\n",
    "\n",
    "**Use when**: You need features in a specific range [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f32770c-11e4-4d17-8314-cc1d67a40c1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# MinMaxScaler - scale to [0,1]\n",
    "minmax_scaler = MinMaxScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features_minmax\"\n",
    ")\n",
    "\n",
    "minmax_model = minmax_scaler.fit(df_assembled)\n",
    "df_minmax = minmax_model.transform(df_assembled)\n",
    "\n",
    "print(\"MinMaxScaler applied (wszystkie values w [0,1]):\")\n",
    "df_minmax.select(\"features_minmax\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4734ad5-6789-4349-b152-0ee67946857d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification: min = 0, max = 1\n",
    "summary = Summarizer.metrics(\"min\", \"max\").summary(col(\"features_minmax\"))\n",
    "display(df_minmax.select(summary.alias(\"summary\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672c7db1-7bca-4da7-9c3b-9e182ffb708a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## RobustScaler - Outlier-Resistant Scaling\n",
    "\n",
    "**Formula**: `(x - median) / IQR`\n",
    "\n",
    "✅ **Używaj gdy**: Dane zawierają outliers (jak w naszym przypadku salary outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5790ee8f-f9e1-4c27-913a-73b6fc7da933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RobustScaler - odporny na outliers\n",
    "robust_scaler = RobustScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features_robust\",\n",
    "    withCentering=True,\n",
    "    withScaling=True\n",
    ")\n",
    "\n",
    "robust_model = robust_scaler.fit(df_assembled)\n",
    "df_robust = robust_model.transform(df_assembled)\n",
    "\n",
    "print(\"RobustScaler applied (median-based, odporny na outliers):\")\n",
    "df_robust.select(\"features_robust\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f9c5322-a896-49cf-890c-7fb7dfe03e27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select necessary columns from each DataFrame to avoid ambiguity\n",
    "df_standard_selected = df_standard.select(\"id\", \"salary\", \"features_standard\")\n",
    "df_robust_selected = df_robust.select(\"id\", \"features_robust\")\n",
    "\n",
    "# Comparison with StandardScaler for salary outliers\n",
    "comparison = df_standard_selected.join(df_robust_selected, \"id\").select(\n",
    "    \"salary\", \"features_standard\", \"features_robust\"\n",
    ").orderBy(col(\"salary\").desc())\n",
    "\n",
    "print(\"Comparison: StandardScaler vs RobustScaler for salary outliers:\")\n",
    "comparison.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "536b2540-a797-4bd3-aa93-c929575e6799",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa06e70-81ea-43b1-ba8a-3d1af99c16b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Normalization - Unit Vector Scaling\n",
    "\n",
    "## Theory\n",
    "**Normalization** scales each row (sample) to a unit vector of length 1.\n",
    "\n",
    "**Use when**: \n",
    "- Features represent similar concepts\n",
    "- Magnitude does not matter, only direction\n",
    "- Text analysis, recommendation systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ea813a0-220c-4998-8d82-5845fa571945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification: each vector has length = 1\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "533ab511-6021-4425-9f40-872747f85da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# L2 Normalization (most commonly used)\n",
    "normalizer = Normalizer(\n",
    "    inputCol=\"features_standard\",  # Using already standardized features\n",
    "    outputCol=\"features_normalized\",\n",
    "    p=2.0  # L2 norm\n",
    ")\n",
    "\n",
    "df_normalized = normalizer.transform(df_standard)\n",
    "\n",
    "display(df_normalized.select(\"features_normalized\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5caa01f-0fbd-48c1-b678-eef102b5887f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753557686976}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def vector_length(v):\n",
    "    return float(math.sqrt(sum(x*x for x in v)))\n",
    "\n",
    "length_udf = udf(vector_length, DoubleType())\n",
    "\n",
    "df_lengths = df_normalized.withColumn(\"vector_length\", length_udf(\"features_normalized\"))\n",
    "display(df_lengths.select(\"vector_length\").describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6f66c37-b293-489c-98cd-42fd192d3a8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## StandardScaler vs Normalizer (L2) – Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a2306db-6027-4e44-b624-27b36bbb8ec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    Row(features=Vectors.dense([10.0, 100.0])),\n",
    "    Row(features=Vectors.dense([20.0, 400.0])),\n",
    "    Row(features=Vectors.dense([30.0, 900.0]))\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aa08a00-848a-410e-9c19-df5cb44bddb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb8492cc-17fe-4212-8f79-655da948424e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "model = scaler.fit(df)\n",
    "df_scaled = model.transform(df)\n",
    "\n",
    "df_scaled.select(\"features\", \"scaled_features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b89fb2f-f461-4e7b-a2e2-2b6574885324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normalized_features\", p=2.0)\n",
    "df_normalized = normalizer.transform(df)\n",
    "\n",
    "df_normalized.select(\"features\", \"normalized_features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7358b3c7-e2c3-4639-add8-d38f4145060a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#String Standardization\n",
    "\n",
    "## Teoria\n",
    "**String Standardization** czyści i ujednolica text data:\n",
    "- **Case normalization** (lower/upper)\n",
    "- **Whitespace cleaning** (trim, multiple spaces)\n",
    "- **Special character handling**\n",
    "- **Consistent formats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dceedac4-f4e0-48a7-b26d-76e55616f4c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# String data analysis before cleaning\n",
    "print(\"\uD83D\uDCCA Raw string data before standardization:\")\n",
    "df_raw.select(\"city\", \"department\").distinct().orderBy(\"city\", \"department\").display(20, truncate=False)\n",
    "\n",
    "print(\"\\n⚠️  Issues:\")\n",
    "print(\"   • Different case (warszawA, KRAKÓW, gdańsk)\")\n",
    "print(\"   • Leading/trailing whitespaces\")\n",
    "print(\"   • Inconsistent formatting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5155b630-3e54-4f99-8939-8cabe0fab8d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# String standardization pipeline\n",
    "df_string_clean = df_raw.select(\n",
    "    \"*\",\n",
    "    # City standardization\n",
    "    trim(lower(col(\"city\"))).alias(\"city_clean\"),\n",
    "    \n",
    "    # Department standardization with additional cleaning\n",
    "    trim(upper(regexp_replace(col(\"department\"), \"\\\\s+\", \" \"))).alias(\"department_clean\")\n",
    ")\n",
    "\n",
    "print(\"String standardization applied:\")\n",
    "display(df_string_clean.select(\"city\", \"city_clean\", \"department\", \"department_clean\").distinct().orderBy(\"city_clean\", \"department_clean\").limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d640e1c2-dfb1-4c66-ae00-96ed80ef7129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bucketizer\n",
    "\n",
    "## Theory\n",
    "**Bucketizer** is a feature engineering tool that transforms a continuous numerical column into discrete bins or \"buckets.\" This process, known as binning, is useful for:\n",
    "\n",
    "- **Handling non-linear relationships:** Converts continuous features into categorical intervals.\n",
    "- **Reducing the effect of outliers:** Groups extreme values into boundary buckets.\n",
    "- **Improving model interpretability:** Makes features easier to understand.\n",
    "\n",
    "### How it works:\n",
    "- Define split points (boundaries) for the buckets.\n",
    "- Each value is assigned to a bucket based on which interval it falls into.\n",
    "- Common use cases: age groups, income ranges, or discretizing scores.\n",
    "\n",
    "**Example:**  \n",
    "A salary column can be bucketized into low, medium, and high salary ranges for downstream analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a8c57a3-48bc-4d9d-a33e-52810a217ea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Example boundaries: <30k, 30k-60k, 60k-100k, >100k\n",
    "salary_splits = [-float(\"inf\"), 30000.0, 60000.0, 100000.0, float(\"inf\")]\n",
    "\n",
    "bucketizer = Bucketizer(\n",
    "    splits=salary_splits,\n",
    "    inputCol=\"salary\",\n",
    "    outputCol=\"salary_bucket\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4fe793e-4fef-4b19-bc33-a65a762322ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assume you have a DataFrame, e.g., `df`\n",
    "df_buckets = bucketizer.transform(df_raw)\n",
    "\n",
    "# Display the result\n",
    "display(df_buckets.select(\"salary\", \"salary_bucket\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49561aa2-93b6-44e8-8ea5-942bab10731a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Outlier Detection and Treatment\n",
    "\n",
    "## Theory\n",
    "**Outliers** can significantly impact model performance. Key methods:\n",
    "\n",
    "### Statistical Methods:\n",
    "- **IQR Method**: Outliers outside Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "- **Z-score**: |z| > 3 (or 2.5)\n",
    "- **Modified Z-score**: Median-based for non-normal distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23003c2f-1c1b-46b3-aa68-5ebfbcc0bff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This block of code detects outliers in the `salary` column using the Interquartile Range (IQR) method, a common technique in data cleaning.\n",
    "\n",
    "### Step-by-step explanation:\n",
    "\n",
    "1. **Calculate quantiles using `percentile_approx()`:**\n",
    "   - **Q1 (25th percentile):** Lower boundary of the middle 50% of values\n",
    "   - **Q3 (75th percentile):** Upper boundary of the middle 50%\n",
    "   - **Median (50th percentile):** The center value (not used for outlier detection, but included for insight)\n",
    "\n",
    "2. **Compute IQR:**\n",
    "   - `IQR = Q3 - Q1`\n",
    "   - The IQR captures the “spread” of the middle 50% of salaries.\n",
    "\n",
    "3. **Define outlier thresholds:**\n",
    "   - **Lower bound:** `Q1 − 1.5 × IQR`\n",
    "   - **Upper bound:** `Q3 + 1.5 × IQR`\n",
    "   - Any value outside this range is considered a potential outlier.\n",
    "\n",
    "4. **Print summary:**\n",
    "   - Displays the computed bounds and IQR statistics in a readable format.\n",
    "\n",
    "---\n",
    "\n",
    "**Why use IQR?**\n",
    "\n",
    "The IQR method is robust to skewed distributions and not sensitive to extreme values, unlike mean-based techniques. It is often used in preprocessing pipelines to flag or remove anomalous data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a215ff-a14c-4e1c-82bc-4822610c7274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# IQR-based outlier detection dla salary\n",
    "salary_stats = df_raw.select(\n",
    "    percentile_approx(\"salary\", 0.25).alias(\"q1\"),\n",
    "    percentile_approx(\"salary\", 0.75).alias(\"q3\"),\n",
    "    percentile_approx(\"salary\", 0.5).alias(\"median\")\n",
    ").collect()[0]\n",
    "\n",
    "q1, q3, median = salary_stats['q1'], salary_stats['q3'], salary_stats['median']\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "print(f\"  Salary outlier bounds (IQR method):\")\n",
    "print(f\"   Q1: {q1:,.0f}, Q3: {q3:,.0f}, IQR: {iqr:,.0f}\")\n",
    "print(f\"   Lower bound: {lower_bound:,.0f}\")\n",
    "print(f\"   Upper bound: {upper_bound:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13238326-0a5c-4ca8-bece-88c64d45188a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Outlier identification\n",
    "df_outliers = df_raw.withColumn(\n",
    "    \"is_outlier\",\n",
    "    when((col(\"salary\") < lower_bound) | (col(\"salary\") > upper_bound), True).otherwise(False)\n",
    ")\n",
    "\n",
    "display(df_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9d29774-c904-48ff-acb8-8903a4868eca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Complete Standardization Pipeline\n",
    "\n",
    "## Połączenie wszystkich technik w jeden production-ready pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3528cc37-ac20-48d9-ada6-5e7abe485213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Complete standardization pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import col, trim, lower, upper, when\n",
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "# Step 1: String cleaning\n",
    "df_pipeline = df_raw.select(\n",
    "    \"*\",\n",
    "    trim(lower(col(\"city\"))).alias(\"city_std\"),\n",
    "    trim(upper(col(\"department\"))).alias(\"department_std\")\n",
    ")\n",
    "\n",
    "# Step 2: Outlier capping\n",
    "df_pipeline = df_pipeline.withColumn(\n",
    "    \"salary_std\",\n",
    "    when(col(\"salary\") > upper_bound, upper_bound)\n",
    "    .when(col(\"salary\") < lower_bound, lower_bound)\n",
    "    .otherwise(col(\"salary\"))\n",
    ")\n",
    "\n",
    "# Step 3: Feature assembly for numerical features\n",
    "numerical_cols_std = [\"age\", \"salary_std\", \"height\", \"score\", \"years_experience\"]\n",
    "assembler_final = VectorAssembler(\n",
    "    inputCols=numerical_cols_std,\n",
    "    outputCol=\"features_raw_final\"\n",
    ")\n",
    "\n",
    "# Step 4: Standard scaling\n",
    "scaler_final = StandardScaler(\n",
    "    inputCol=\"features_raw_final\",\n",
    "    outputCol=\"features_final\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "# Pipeline execution\n",
    "df_assembled_final = assembler_final.transform(df_pipeline)\n",
    "scaler_model_final = scaler_final.fit(df_assembled_final)\n",
    "df_final = scaler_model_final.transform(df_assembled_final)\n",
    "\n",
    "print(\"Complete standardization pipeline executed:\")\n",
    "display(df_final.select(\"city_std\", \"department_std\", \"salary_std\", \"features_final\").limit(10))\n",
    "\n",
    "# Final validation\n",
    "print(\"Final standardized features statistics:\")\n",
    "summary_final = Summarizer.metrics(\"mean\", \"std\", \"min\", \"max\").summary(col(\"features_final\"))\n",
    "display(df_final.select(summary_final.alias(\"summary\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1b5ae6-704d-4085-868b-409b75cae8fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Summary and Best Practices\n",
    "\n",
    "## When to use each standardization method?\n",
    "\n",
    "### 1️⃣ **StandardScaler**\n",
    " **Use when:**\n",
    "- Features have a **normal distribution**\n",
    "- **Linear models** (logistic regression, SVM)\n",
    "- **Neural networks** (gradient descent optimization)\n",
    "- **PCA** and other dimensionality reduction\n",
    "\n",
    "### 2️⃣ **MinMaxScaler**\n",
    " **Use when:**\n",
    "- You need a **bounded range** [0,1]\n",
    "- **Image processing** (pixel values)\n",
    "- **Neural networks** with activation functions\n",
    "- **Sparse data** (preserve zero values)\n",
    "\n",
    "### 3️⃣ **RobustScaler**\n",
    " **Use when:**\n",
    "- Data contains **outliers**\n",
    "- **Non-normal distributions**\n",
    "- **Robust algorithms** (tree-based often do not require scaling)\n",
    "\n",
    "### 4️⃣ **Normalizer**\n",
    " **Use when:**\n",
    "- **Text analysis** (TF-IDF vectors)\n",
    "- **Recommendation systems**\n",
    "- Direction matters more than magnitude\n",
    "\n",
    "##  Key principles\n",
    "\n",
    "1. **Fit scaler only on training data** - prevent data leakage!\n",
    "2. **Apply transformation to train/validation/test** with the same scaler\n",
    "3. **Handle outliers before scaling** for better results\n",
    "4. **Standardize strings** for consistent categorical encoding\n",
    "5. **Tree-based algorithms** often do not require feature scaling\n",
    "6. **Save scaler models** for production deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7edc204-06a5-4289-aaa6-b8f1df259b8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## QUICK REFERENCE for Databricks ML Associate\n",
    "\n",
    "### **1️⃣ StandardScaler:**\n",
    "```python\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaled', withMean=True, withStd=True)\n",
    "model = scaler.fit(train_df)\n",
    "scaled_df = model.transform(df)\n",
    "```\n",
    "\n",
    "### **2️⃣ MinMaxScaler:**\n",
    "```python\n",
    "scaler = MinMaxScaler(inputCol='features', outputCol='scaled')\n",
    "```\n",
    "\n",
    "### **3️⃣ String cleaning:**\n",
    "```python\n",
    "df.withColumn('clean', trim(lower(col('text_col'))))\n",
    "```\n",
    "\n",
    "### **4️⃣ Outlier detection (IQR):**\n",
    "```python\n",
    "lower = q1 - 1.5 * iqr\n",
    "upper = q3 + 1.5 * iqr\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've completed Data Standardization in Databricks ML**  \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Data_Standarization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}