{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da7fcc91-dde0-49dc-81bb-2e6eb289d79a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Data Preparation in ML - Notebook 06\n",
    "## Data Splitting Fundamentals\n",
    "\n",
    "**Part of the Databricks Data Preparation in ML Training Series**\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook covers essential data splitting techniques required for Databricks ML Associate Certification:\n",
    "\n",
    "- **Basic Split** - Train/Test (80/20) for simple model validation\n",
    "- **Three-way Split** - Train/Validation/Test (60/20/20) for hyperparameter tuning\n",
    "- **Cross-Validation** - K-fold strategies for maximum data utilization\n",
    "- **Stratified Sampling** - Maintaining class distributions across splits\n",
    "- **Time-based Splitting** - Temporal considerations for time series data\n",
    "\n",
    "## Duration: ~45 minutes\n",
    "## Level: Fundamental → Intermediate\n",
    "\n",
    "---\n",
    "\n",
    "## Why is Proper Data Splitting Critical?\n",
    "\n",
    "Proper data splitting forms the **foundation of every ML project**:\n",
    "- **Objective Evaluation** - Avoiding overfitting and ensuring model reliability\n",
    "- **Generalization** - Ensuring models perform well on unseen data\n",
    "- **Production Readiness** - Simulating real-world deployment scenarios\n",
    "- **Fair Comparison** - Enabling objective comparison between different models\n",
    "\n",
    "---\n",
    "\n",
    "## Theory: Data Splitting Strategies\n",
    "\n",
    "### Basic Train/Test Split (80/20)\n",
    "```\n",
    "Dataset → Train (80%) + Test (20%)\n",
    "    ↓        ↓            ↓\n",
    "Training   Model       Final\n",
    " Data      Fitting    Evaluation\n",
    "```\n",
    "- **Use case**: Simple model validation\n",
    "- **Pros**: Simple, fast, sufficient for large datasets\n",
    "- **Cons**: Limited evaluation, no hyperparameter tuning\n",
    "\n",
    "### Three-way Split (60/20/20)\n",
    "```\n",
    "Dataset → Train (60%) + Validation (20%) + Test (20%)\n",
    "    ↓        ↓              ↓              ↓\n",
    "Training   Model      Hyperparameter    Final\n",
    " Data      Fitting      Tuning        Evaluation\n",
    "```\n",
    "- **Use case**: Model selection and hyperparameter optimization\n",
    "- **Pros**: Unbiased final evaluation, enables tuning\n",
    "- **Cons**: Reduces training data size\n",
    "\n",
    "### \uD83D\uDD04 Cross-Validation (K-fold)\n",
    "```\n",
    "Fold 1: [TEST ] [TRAIN] [TRAIN] [TRAIN] [TRAIN]\n",
    "Fold 2: [TRAIN] [TEST ] [TRAIN] [TRAIN] [TRAIN]\n",
    "Fold 3: [TRAIN] [TRAIN] [TEST ] [TRAIN] [TRAIN]\n",
    "...\n",
    "Final Score = Average(Fold1, Fold2, ..., FoldK)\n",
    "```\n",
    "- **Use case**: Robust model evaluation with limited data\n",
    "- **Pros**: Maximum data utilization, robust estimates\n",
    "- **Cons**: Computationally expensive (K times training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aa3244f-910e-4a5f-b164-e06241b5bdcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f960f53-849f-4593-82d9-4877634ff29a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Basic imports for Databricks ML\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, row_number\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9087c7f-7a9d-46ab-b1d9-2069882625e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create demonstration dataset - Customer Churn\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate customer data\n",
    "n_customers = 1000\n",
    "ages = np.random.normal(45, 15, n_customers).clip(18, 80)\n",
    "incomes = np.random.lognormal(10.5, 0.6, n_customers)\n",
    "monthly_charges = np.random.normal(65, 25, n_customers).clip(10, 150)\n",
    "support_calls = np.random.poisson(2, n_customers)\n",
    "\n",
    "# Create realistic churn probability\n",
    "churn_prob = (\n",
    "    0.3 * (support_calls > 3) + \n",
    "    0.25 * (monthly_charges > 80) + \n",
    "    0.2 * (ages < 30) + \n",
    "    0.1 * np.random.random(n_customers)\n",
    ")\n",
    "churn = (churn_prob > 0.4).astype(int)\n",
    "\n",
    "# Schema and data\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"age\", DoubleType(), True),\n",
    "    StructField(\"income\", DoubleType(), True),\n",
    "    StructField(\"monthly_charges\", DoubleType(), True),\n",
    "    StructField(\"support_calls\", IntegerType(), True),\n",
    "    StructField(\"churn\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [(i, float(ages[i]), float(incomes[i]), float(monthly_charges[i]), \n",
    "         int(support_calls[i]), int(churn[i])) for i in range(n_customers)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.display(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbdcf539-1a28-43f9-9c1f-ce45588bd86f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Basic Split - Train/Test (80/20)\n",
    "\n",
    "## Theory\n",
    "**Basic Split** divides data into two parts: training (80%) and test (20%).\n",
    "\n",
    "###  When to use:\n",
    "- **Large datasets** (>10,000 samples)\n",
    "- **Simple models** without hyperparameter tuning\n",
    "- **Quick prototyping** and baseline models\n",
    "\n",
    "###  Advantages:\n",
    "- **Simplicity** - easy to implement\n",
    "- **Speed** - minimal overhead\n",
    "- **Clarity** - clear structure\n",
    "\n",
    "###  Limitations:\n",
    "- **No tuning** - no validation set\n",
    "- **Variance** - results depend on random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "321f046f-3944-4385-a2ce-28c0fb1a428a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare features for the model\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define input columns\n",
    "feature_cols = ['age', 'income', 'monthly_charges', 'support_calls']\n",
    "\n",
    "# Create VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "# Transform data\n",
    "df_features = assembler.transform(df)\n",
    "\n",
    "print(\"Data with feature vectors:\")\n",
    "df_features.select(\"customer_id\", \"features\", \"churn\").display(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccc9f88e-6e77-44e7-839b-8a0801217c78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split into training and test sets (80/20)\n",
    "train_data, test_data = df_features.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set size: {train_data.count()}\")\n",
    "print(f\"Test set size: {test_data.count()}\")\n",
    "\n",
    "# Check target class distribution\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "train_data.groupBy(\"churn\").count().display()\n",
    "\n",
    "print(\"Class distribution in test set:\")\n",
    "test_data.groupBy(\"churn\").count().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f2d15dd-ee15-4cde-a9a2-351553f83318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Churn (customer churn) refers to the loss of customers or users over time. In a business or ML context, churn typically means that a customer:\n",
    "\n",
    "\t•\tcancels a subscription,\n",
    "\t•\tstops using a product or service,\n",
    "\t•\tbecomes inactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fefdea36-8f49-4dbf-be5b-77bd28bdf41c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train binary classification model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create model\n",
    "lr = LogisticRegression(\n",
    "    featuresCol='features', \n",
    "    labelCol='churn',\n",
    "    maxIter=10\n",
    ")\n",
    "\n",
    "# Train model on training set\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "print(\"Model trained on training set\")\n",
    "print(f\"Number of iterations: {lr_model.summary.totalIterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "024c1023-d415-4308-a4f7-79322db7f4c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Check predictions\n",
    "print(\"Predictions on test set:\")\n",
    "predictions.select(\"customer_id\", \"churn\", \"prediction\", \"probability\").show(10)\n",
    "\n",
    "# Calculate accuracy\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"churn\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"\\nAUC on test set: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "250b61ba-e6f8-4bd0-a159-7f5c4d4bb92d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#  Three-way Split - Train/Validation/Test (60/20/20)\n",
    "\n",
    "## Theory\n",
    "\n",
    "**Three-way Split** divides data into **three independent sets**:\n",
    "\n",
    "- **Training Set (60%)**: For training the model\n",
    "- **Validation Set (20%)**: For hyperparameter tuning and model selection\n",
    "- **Test Set (20%)**: For final model evaluation\n",
    "\n",
    "### Why do we use three sets?\n",
    "\n",
    "1. **Training Set**: Model learns patterns in the data\n",
    "2. **Validation Set**: We test different model configurations without \"looking\" at the test set\n",
    "3. **Test Set**: Final, objective evaluation of model performance\n",
    "\n",
    "### Process:\n",
    "1. Train model on **Training Set**\n",
    "2. Evaluate different hyperparameters on **Validation Set**\n",
    "3. Select the best configuration\n",
    "4. Final evaluation on **Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "119e2183-2efa-4277-9113-3b7859b3cea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split into three sets: train (60%), validation (20%), test (20%)\n",
    "train_data_3way, temp_data = df_features.randomSplit([0.6, 0.4], seed=42)\n",
    "val_data, test_data_3way = temp_data.randomSplit([0.5, 0.5], seed=42)\n",
    "\n",
    "print(f\"Training set: {train_data_3way.count()} records\")\n",
    "print(f\"Validation set: {val_data.count()} records\") \n",
    "print(f\"Test set: {test_data_3way.count()} records\")\n",
    "\n",
    "# Check proportions\n",
    "total = train_data_3way.count() + val_data.count() + test_data_3way.count()\n",
    "print(f\"\\nProportions:\")\n",
    "print(f\"Train: {train_data_3way.count()/total:.1%}\")\n",
    "print(f\"Validation: {val_data.count()/total:.1%}\")\n",
    "print(f\"Test: {test_data_3way.count()/total:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e241bb5b-ff53-43e5-b310-6fd1b698c433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test different hyperparameters\n",
    "hyperparams = [1, 5, 10, 20]\n",
    "best_auc = 0\n",
    "best_maxIter = None\n",
    "\n",
    "print(\"Hyperparameter tuning on validation set:\")\n",
    "\n",
    "for maxIter in hyperparams:\n",
    "    # Train model\n",
    "    lr = LogisticRegression(\n",
    "        featuresCol='features', \n",
    "        labelCol='churn',\n",
    "        maxIter=maxIter\n",
    "    )\n",
    "    \n",
    "    model = lr.fit(train_data_3way)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_predictions = model.transform(val_data)\n",
    "    val_auc = evaluator.evaluate(val_predictions)\n",
    "    \n",
    "    print(f\"maxIter={maxIter}: AUC = {val_auc:.3f}\")\n",
    "    \n",
    "    # Save best result\n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        best_maxIter = maxIter\n",
    "\n",
    "print(f\"Best hyperparameter: maxIter={best_maxIter} (AUC={best_auc:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48b39ba-14e5-4160-bfd1-990d58dbe7a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train final model with best hyperparameters\n",
    "final_lr = LogisticRegression(\n",
    "    featuresCol='features', \n",
    "    labelCol='churn',\n",
    "    maxIter=best_maxIter\n",
    ")\n",
    "\n",
    "final_model = final_lr.fit(train_data_3way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78b78788-fb8f-44ec-977e-ad5b967443a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "test_predictions = final_model.transform(test_data_3way)\n",
    "final_auc = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(f\"Final model performance on test set:\")\n",
    "print(f\"AUC = {final_auc:.3f}\")\n",
    "\n",
    "# Comparison with validation set\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"Validation AUC: {best_auc:.3f}\")\n",
    "print(f\"Test AUC: {final_auc:.3f}\")\n",
    "print(f\"Difference: {abs(final_auc - best_auc):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "872c9d61-8e56-49f1-8c80-57026c779181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#  Cross-Validation (K-Fold)\n",
    "\n",
    "## Theory\n",
    "\n",
    "**Cross-Validation** is a technique that **maximizes data utilization** by repeatedly training and testing the model on different subsets.\n",
    "\n",
    "### K-Fold Cross-Validation:\n",
    "\n",
    "1. **Split the data into K parts** (e.g., K=5)\n",
    "2. **Train the model K times**:\n",
    "   - Each time, use K-1 parts for training\n",
    "   - 1 part is used for testing\n",
    "3. **Average the results** from all K iterations\n",
    "\n",
    "### Advantages:\n",
    "-  **Better data utilization** - every record is used for both training and testing\n",
    "-  **More stable evaluation** - averaging reduces the impact of randomness\n",
    "-  **Overfitting detection** - high variance between folds may indicate a problem\n",
    "\n",
    "### Disadvantages:\n",
    "-  **Computationally expensive** - training is performed K times\n",
    "-  **Longer duration** - especially for large models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1b9e99-f8d9-4522-8b76-c2729b1140ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We use a logistic regression classifier to predict churn (labelCol='churn') based on input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5221f0b6-b594-49e2-995b-b8e71641b94f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cross-Validation with Spark ML\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Define model\n",
    "cv_lr = LogisticRegression(featuresCol='features', labelCol='churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d84a75c2-af72-4d93-8e07-98ededfc7172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This creates a grid of hyperparameter combinations to test during training:\n",
    "\n",
    "maxIter: number of training iterations (gradient descent steps)\n",
    "\n",
    "regParam: regularization strength (higher = stronger regularization)\n",
    "\n",
    "With 3 values for each parameter, there are 3 × 3 = 9 combinations to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3798101-a844-4d04-bc0b-515454c13c23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter grid to test\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(cv_lr.maxIter, [5, 10, 20]) \\\n",
    "    .addGrid(cv_lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Number of hyperparameter combinations: {len(paramGrid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21b83253-678b-47b4-94e5-883ca4d27f0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "estimator: the ML model to train (logistic regression)\n",
    "\n",
    "estimatorParamMaps: the grid of parameter combinations\n",
    "\n",
    "evaluator: a metric evaluator (e.g., BinaryClassificationEvaluator)\n",
    "\n",
    "numFolds=3: 3-fold cross-validation\n",
    "\n",
    "\n",
    "➜ dataset is split into 3 parts, and each part is used once as validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f12c1131-0550-45e1-a74b-21566f570045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure Cross-Validator\n",
    "crossval = CrossValidator(\n",
    "    estimator=cv_lr,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,  # 3-fold CV for faster execution\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Cross-Validator configured (3-fold CV)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a10029cd-9f87-490f-8b17-ff91c15fa906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split data into train and test for CV\n",
    "cv_train_data, cv_test_data = df_features.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"Starting Cross-Validation...\")\n",
    "print(\"This may take a moment...\")\n",
    "\n",
    "# Training with Cross-Validation\n",
    "cv_model = crossval.fit(cv_train_data)\n",
    "\n",
    "print(\"Cross-Validation completed!\")\n",
    "print(f\"Best model selected from {len(paramGrid)} combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5dafc35-042c-4ee4-9d4c-0b512ac0d135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze Cross-Validation results\n",
    "best_model = cv_model.bestModel\n",
    "best_params = {\n",
    "    \"maxIter\": best_model.getMaxIter(),\n",
    "    \"regParam\": best_model.getRegParam()\n",
    "}\n",
    "\n",
    "print(\"Best hyperparameters from CV:\")\n",
    "print(f\"maxIter: {best_params['maxIter']}\")\n",
    "print(f\"regParam: {best_params['regParam']}\")\n",
    "\n",
    "# Average results from all combinations\n",
    "avg_metrics = cv_model.avgMetrics\n",
    "#Area Under the Curve\n",
    "print(f\"Best average AUC from CV: {max(avg_metrics):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d730e49-fc0e-4be5-954a-56fd2071a15d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Final Notes\n",
    "\n",
    "- Choose your data splitting strategy based on dataset size, model complexity, and evaluation needs.\n",
    "- Always keep the test set untouched until the very end for unbiased performance assessment.\n",
    "- Use cross-validation for small datasets or when you need robust, stable metrics.\n",
    "- Set random seeds for reproducibility.\n",
    "- For classification, use stratified splits to maintain class balance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "312e8c92-7c40-4298-a71d-b05e7cb107c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Final evaluation of best model from CV\n",
    "cv_test_predictions = cv_model.transform(cv_test_data)\n",
    "cv_test_auc = evaluator.evaluate(cv_test_predictions)\n",
    "\n",
    "print(f\"Final evaluation of Cross-Validation model:\")\n",
    "print(f\"Test AUC: {cv_test_auc:.3f}\")\n",
    "\n",
    "# Comparison of all three methods\n",
    "print(f\"\\n\uD83D\uDCCA Comparison of all methods:\")\n",
    "print(f\"Basic Split AUC:     {auc:.3f}\")\n",
    "print(f\"Three-way Split AUC: {final_auc:.3f}\")\n",
    "print(f\"Cross-Validation AUC: {cv_test_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5988b5f8-2dfa-43b9-aeaa-ce0dd41cdf66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Summary: When to use which method?\n",
    "\n",
    "## Decision Framework\n",
    "\n",
    "### 1️⃣ **Basic Split (80/20)** \n",
    "✅ **Use when:**\n",
    "- You have a **large dataset** (>10,000 samples)\n",
    "- Model is **simple** without complex hyperparameters\n",
    "- You need a **quick prototype** or baseline\n",
    "- **Time** is limited\n",
    "\n",
    "### 2️⃣ **Three-way Split (60/20/20)**\n",
    "✅ **Use when:**\n",
    "- You have a **medium dataset** (1,000-50,000 samples)\n",
    "- You plan **hyperparameter tuning**\n",
    "- You want to **compare different models**\n",
    "- You need **objective final evaluation**\n",
    "\n",
    "### 3️⃣ **Cross-Validation**\n",
    "✅ **Use when:**\n",
    "- You have a **small dataset** (<5,000 samples)\n",
    "- You want to **maximize data utilization**\n",
    "- You need **stable metrics**\n",
    "- **Computation time** is not critical\n",
    "\n",
    "## ⚠️ Key principles\n",
    "\n",
    "1. **Never use test set for hyperparameter tuning!**\n",
    "2. **Always set random seed for reproducibility**\n",
    "3. **Check class distribution in each set**\n",
    "4. **For imbalanced data use stratified split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d7b1d27-ccca-4d1b-975c-143858921a75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Quick Reference for Databricks ML Associate\n",
    "\n",
    "## Essential Data Splitting Code Patterns\n",
    "\n",
    "### 1️⃣ Basic Train/Test Split\n",
    "```python\n",
    "# Simple 80/20 split with reproducible results\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "```\n",
    "\n",
    "### 2️⃣ Three-way Split (Train/Validation/Test)\n",
    "```python\n",
    "# First split: 60% train, 40% temp\n",
    "train, temp = df.randomSplit([0.6, 0.4], seed=42)\n",
    "\n",
    "# Second split: 20% validation, 20% test\n",
    "val, test = temp.randomSplit([0.5, 0.5], seed=42)\n",
    "```\n",
    "\n",
    "### 3️⃣ Cross-Validation\n",
    "```python\n",
    "# K-fold cross-validation for robust evaluation\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "cv = CrossValidator(estimator=model, evaluator=eval, numFolds=5)\n",
    "```\n",
    "\n",
    "### 4️⃣ Stratified Split (for Classification)\n",
    "```python\n",
    "# Maintain class distribution across splits\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# For categorical targets, ensure balanced representation\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You have completed **Data Splitting in Databricks ML**!  \n",
    "\n",
    "\n",
    "### Key Skills Acquired:\n",
    "- ✅ Proper train/validation/test split strategies\n",
    "- ✅ Cross-validation implementation techniques\n",
    "- ✅ Stratified sampling for balanced datasets\n",
    "- ✅ Reproducible data splitting with random seeds\n",
    "- ✅ Best practices for unbiased model evaluation"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_Data_Splitting",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}