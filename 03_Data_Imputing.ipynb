{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8bb566c-9a5a-4491-8467-d53655e04c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Data Preparation in ML - Notebook 03\n",
    "## Data Imputation Fundamentals\n",
    "\n",
    "**Part of the Databricks Data Preparation in ML Training Series**\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook covers essential missing data handling techniques required for Databricks ML Associate Certification:\n",
    "\n",
    "- **Missing Data Mechanisms** - Understanding MCAR, MAR, and MNAR patterns\n",
    "- **Deletion Methods** - Listwise and pairwise deletion strategies\n",
    "- **Simple Imputation** - Mean, median, mode, and constant value imputation\n",
    "- **Advanced Imputation** - KNN, regression-based, and iterative methods\n",
    "- **Evaluation Strategies** - Assessing imputation quality and impact\n",
    "\n",
    "## Duration: ~45 minutes\n",
    "## Level: Fundamental → Intermediate\n",
    "\n",
    "---\n",
    "\n",
    "## Why is Proper Missing Data Handling Critical?\n",
    "\n",
    "Missing data is a **common challenge in real-world datasets** that can significantly impact ML model performance:\n",
    "- **Bias Reduction** - Avoiding systematic errors in model training\n",
    "- **Statistical Power** - Preserving the reliability of statistical inference\n",
    "- **Model Performance** - Ensuring optimal algorithm performance\n",
    "- **Production Readiness** - Maintaining stability in production environments\n",
    "\n",
    "---\n",
    "\n",
    "## Theory: Missing Data Mechanisms\n",
    "\n",
    "Understanding the mechanism behind missing data is crucial for choosing the appropriate imputation strategy.\n",
    "\n",
    "### Missing Completely at Random (MCAR)\n",
    "```\n",
    "Missingness is completely random and independent of observed/unobserved data\n",
    "P(Missing | Observed, Unobserved) = P(Missing)\n",
    "Example: Sensor malfunction due to random hardware failures\n",
    "```\n",
    "- **Characteristics**: Easiest to handle, least biased\n",
    "- **Detection**: Little's MCAR test\n",
    "- **Strategy**: Any imputation method is valid\n",
    "\n",
    "### Missing at Random (MAR)\n",
    "```\n",
    "Missingness depends on observed data but not on unobserved values\n",
    "P(Missing | Observed, Unobserved) = P(Missing | Observed)\n",
    "Example: Older customers less likely to provide income information\n",
    "```\n",
    "- **Characteristics**: Can be predicted from other variables\n",
    "- **Detection**: Analysis of missing patterns vs observed variables\n",
    "- **Strategy**: Use observed data to inform imputation\n",
    "\n",
    "### Missing Not at Random (MNAR)\n",
    "```\n",
    "Missingness depends on the unobserved values themselves\n",
    "P(Missing | Observed, Unobserved) = P(Missing | Unobserved)\n",
    "Example: High earners deliberately withholding income information\n",
    "```\n",
    "- **Characteristics**: Most challenging, requires domain knowledge\n",
    "- **Detection**: Often requires subject matter expertise\n",
    "- **Strategy**: Model the missingness mechanism explicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a9c28c4-21e1-4c64-8d0f-bca284ec3687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c8ce1f0-061e-4211-8ef9-d6c523e87f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Basic imports for Databricks ML\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, isnan, isnull, rand, randn, lit, mean, stddev\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1bfea5f-4592-49fd-be13-e152f21c4612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating demonstration dataset with missing data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generating patient data with different types of missing values\n",
    "n_patients = 1000\n",
    "ages = np.random.normal(45, 15, n_patients).clip(18, 80)\n",
    "weights = np.random.normal(70, 12, n_patients).clip(40, 120)\n",
    "heights = np.random.normal(170, 10, n_patients).clip(150, 200)\n",
    "blood_pressure = np.random.normal(120, 20, n_patients).clip(80, 180)\n",
    "incomes = np.random.lognormal(10, 0.8, n_patients)\n",
    "\n",
    "# Schema and data\n",
    "schema = StructType([\n",
    "    StructField(\"patient_id\", IntegerType(), True),\n",
    "    StructField(\"age\", DoubleType(), True),\n",
    "    StructField(\"weight\", DoubleType(), True),\n",
    "    StructField(\"height\", DoubleType(), True),\n",
    "    StructField(\"blood_pressure\", DoubleType(), True),\n",
    "    StructField(\"income\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "data = [(i, float(ages[i]), float(weights[i]), float(heights[i]), \n",
    "         float(blood_pressure[i]), float(incomes[i])) for i in range(n_patients)]\n",
    "\n",
    "df_complete = spark.createDataFrame(data, schema)\n",
    "df_complete.display(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d16faf-7c73-4f73-a0b7-1b8597bc0905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Missing Data Mechanisms\n",
    "\n",
    "## Theory\n",
    "**Missing Data Mechanisms** define why data is missing. Understanding the mechanism is crucial for choosing the right imputation strategy.\n",
    "\n",
    "### Types of Missing Data:\n",
    "- **MCAR** (Missing Completely at Random): Missing data is independent of observed and unobserved values\n",
    "- **MAR** (Missing at Random): Missing data depends only on observed values\n",
    "- **MNAR** (Missing Not at Random): Missing data depends on unobserved values\n",
    "\n",
    "### Identification methods:\n",
    "- **MCAR**: Statistical tests (Little's MCAR test)\n",
    "- **MAR**: Analysis of missing patterns vs observed variables  \n",
    "- **MNAR**: Domain knowledge and business logic\n",
    "\n",
    "### Impact on strategy:\n",
    "- **MCAR**: Can safely delete or impute\n",
    "- **MAR**: Imputation using observed variables\n",
    "- **MNAR**: Advanced methods or mechanism modeling needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11051337-556d-45e9-be0d-80c91cec4e28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MCAR: Random Missing Values\n",
    "\n",
    "Missing Completely at Random (MCAR) occurs when the probability of missing data on a variable is unrelated to any other measured or unmeasured variable. In other words, the missingness is entirely random and does not depend on observed or unobserved data. This mechanism is the easiest to handle, as any imputation or deletion method will not introduce bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f3bbdd8-4b93-492e-b8e3-868935c96696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulation of different missing data mechanisms\n",
    "\n",
    "# MCAR: Random missing values in weight (10%)\n",
    "df_mcar = df_complete.withColumn(\n",
    "    \"weight\",\n",
    "    when(rand() < 0.1, None).otherwise(col(\"weight\"))\n",
    ")\n",
    "\n",
    "print(\"MCAR - Random missing values in weight:\")\n",
    "missing_weight = df_mcar.filter(col(\"weight\").isNull()).count()\n",
    "print(f\"Missing weight values: {missing_weight} ({missing_weight/n_patients:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab15ac51-eb35-4825-85ef-1a638a236eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MAR: Missing at Random\n",
    "\n",
    "Missing at Random (MAR) occurs when the probability of missing data on a variable is related to other observed variables, but not to the value of the variable itself. In this case, the missingness can be explained by information available in the dataset. Imputation methods that use observed data, such as regression or MICE, are appropriate for handling MAR.\n",
    "\n",
    "**Example:** Income data is more likely to be missing for older individuals, but among people of the same age, missingness is random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a141f8f-2c13-41ef-a5f7-9f0fb9be612f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAR: Missing income dependent on age (older people don't provide income)\n",
    "df_mar = df_mcar.withColumn(\n",
    "    \"income\",\n",
    "    when((col(\"age\") > 60) & (rand() < 0.3), None)\n",
    "    .when((col(\"age\") <= 60) & (rand() < 0.05), None)\n",
    "    .otherwise(col(\"income\"))\n",
    ")\n",
    "\n",
    "print(\"MAR - Missing income dependent on age:\")\n",
    "missing_income = df_mar.filter(col(\"income\").isNull()).count()\n",
    "print(f\"Missing income values: {missing_income} ({missing_income/n_patients:.1%})\")\n",
    "\n",
    "# Check dependency\n",
    "print(\"Distribution of missing income by age:\")\n",
    "df_mar.select(\n",
    "    when(col(\"age\") <= 60, \"Young\").otherwise(\"Old\").alias(\"age_group\"),\n",
    "    col(\"income\").isNull().alias(\"income_missing\")\n",
    ").groupBy(\"age_group\", \"income_missing\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdbfd3e1-4cf7-495b-8492-3cf91402d9ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MNAR: Missing Not at Random\n",
    "\n",
    "Missing Not at Random (MNAR) occurs when the probability of missing data on a variable is related to the unobserved value itself. In this case, the reason for missingness is directly tied to the missing data, making it the most challenging mechanism to address. Handling MNAR often requires explicit modeling of the missingness process or incorporating domain expertise.\n",
    "\n",
    "**Example:** High-income individuals are less likely to report their income, so missingness depends on the (unseen) income value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea26bef-2000-49c3-b301-002e18731765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MNAR: Missing blood pressure - people with high blood pressure avoid tests\n",
    "df_mnar = df_mar.withColumn(\n",
    "    \"blood_pressure_original\", col(\"blood_pressure\")\n",
    ").withColumn(\n",
    "    \"blood_pressure\",\n",
    "    when((col(\"blood_pressure\") > 140) & (rand() < 0.4), None)\n",
    "    .when((col(\"blood_pressure\") <= 140) & (rand() < 0.05), None) \n",
    "    .otherwise(col(\"blood_pressure\"))\n",
    ")\n",
    "\n",
    "print(\"MNAR - Missing blood pressure (high blood pressure = more missing):\")\n",
    "missing_bp = df_mnar.filter(col(\"blood_pressure\").isNull()).count()\n",
    "print(f\"Missing blood pressure values: {missing_bp} ({missing_bp/n_patients})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea62da43-cfa3-4365-ab34-d6e0bcb25e11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check bias\n",
    "print(\"\\nMean blood pressure before and after introducing missing values:\")\n",
    "orig_mean = df_mnar.agg(mean(\"blood_pressure_original\")).collect()[0][0]\n",
    "observed_mean = df_mnar.agg(mean(\"blood_pressure\")).collect()[0][0]\n",
    "print(f\"Original mean: {orig_mean}\")\n",
    "print(f\"Observed mean: {observed_mean}\")\n",
    "print(f\"Bias: {orig_mean - observed_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b356ed49-b5f9-48e9-a718-ef3c205baba2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_missing_data = df_mnar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb074f8d-0791-4f65-8d19-4d2a9e8c245e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc1f8b2c-c828-40c3-8d43-3cb8aee8c1d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Deletion Methods\n",
    "\n",
    "## Theory\n",
    "\n",
    "**Deletion Methods** remove observations or variables with missing data. This is the simplest approach but can lead to loss of information.\n",
    "\n",
    "### Listwise Deletion (Complete Case Analysis)\n",
    "- **Removes entire rows** with any missing values\n",
    "-  **Advantages**: Simple, preserves relationships between variables\n",
    "-  **Disadvantages**: Can remove a lot of data, bias if missing data is not MCAR\n",
    "\n",
    "###  Pairwise Deletion\n",
    "- **Uses available data** for each analysis\n",
    "-  **Advantages**: Preserves more data\n",
    "-  **Disadvantages**: Different sample sizes, potential correlation problems\n",
    "\n",
    "###  Variable Deletion\n",
    "- **Removes variables** with high proportion of missing values\n",
    "-  **Advantages**: Preserves observations\n",
    "-  **Disadvantages**: Loss of potentially important variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fefd99e-ce23-4b79-ba8c-481f2a695cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "021db5da-48e6-4281-8522-0003fb8182ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Missing Data Pattern Analysis - Drop Not Needed Column\n",
    "\n",
    "Before analyzing missing data patterns, it's important to remove columns that are irrelevant or not needed for your analysis. Dropping unnecessary columns helps focus on meaningful variables, reduces noise, and improves the clarity of missing data visualizations and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e05d740c-4a3e-4d31-9f2a-634edaced976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_missing_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e7fbbe1-f8e3-4403-856b-910a3629e98c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Missing data pattern analysis\n",
    "df_missing = df_missing_data.drop(\"blood_pressure_original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08430116-f2b0-4d91-8cb7-7876393b47e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdc533ba-dbbc-4dda-b88a-81421ddb6a25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9108a8bf-7f36-4480-bf38-ea49744029a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for col_name in df_missing.columns:\n",
    "    if col_name != \"patient_id\":\n",
    "        missing_count = df_missing.filter(col(col_name).isNull()).count()\n",
    "        missing_pct = missing_count / n_patients * 100\n",
    "        print(f\"{col_name:15s}: {missing_count:4d} ({missing_pct:5.1f}%)\")\n",
    "\n",
    "# Complete Case Analysis (Listwise Deletion)\n",
    "df_complete_cases = df_missing.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0133cfb0-dc4f-4208-aec1-991f61cf0e58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Listwise Deletion:\")\n",
    "original_count = df_missing.count()\n",
    "complete_count = df_complete_cases.count()\n",
    "removed_count = original_count - complete_count\n",
    "\n",
    "print(f\"Original observations: {original_count}\")\n",
    "print(f\"Complete cases: {complete_count}\")\n",
    "print(f\"Removed observations: {removed_count} ({removed_count/original_count:.1%})\")\n",
    "\n",
    "print(f\"Retained data: {complete_count/original_count:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c7cf4de-5d7b-4528-a683-8090b115a275",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753546105621}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_complete_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d430640f-1c87-469f-bbf1-3ac5213575cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Variable Deletion - remove variables with >20% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0a0d216-e828-441c-b684-f4917e5deafb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Variable Deletion - remove variables if is missing more then threshold %\n",
    "threshold = 0.10\n",
    "cols_to_keep = [\"patient_id\"]\n",
    "\n",
    "for col_name in df_missing.columns:\n",
    "    if col_name != \"patient_id\":\n",
    "        missing_pct = df_missing.filter(col(col_name).isNull()).count() / n_patients\n",
    "        if missing_pct <= threshold:\n",
    "            cols_to_keep.append(col_name)\n",
    "        else:\n",
    "            print(f\"Removing {col_name}: {missing_pct:.1%} missing\")\n",
    "\n",
    "df_var_deleted = df_missing.select(*cols_to_keep)\n",
    "\n",
    "print(f\"Retained columns: {cols_to_keep[1:]}\")\n",
    "print(f\"Retained variables: {len(cols_to_keep)-1}/{len(df_missing.columns)-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "035d76b2-6b10-48a6-9624-dc377d4ea40e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753545478418}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_var_deleted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef80f22a-d760-45eb-8318-0beb321d183b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Simple Imputation Methods\n",
    "\n",
    "## Theory\n",
    "\n",
    "**Simple Imputation** replaces missing values with single values based on simple statistics.\n",
    "\n",
    "### Main Strategies:\n",
    "- **Mean/Median**: For numerical variables\n",
    "- **Mode**: For categorical variables  \n",
    "- **Constant**: Fixed value (0, \"Unknown\", etc.)\n",
    "- **Forward/Backward Fill**: Use previous/next values\n",
    "\n",
    "### Advantages:\n",
    "- Fast and simple\n",
    "- Preserves all observations\n",
    "- Easy to implement\n",
    "\n",
    "### Disadvantages:\n",
    "- Reduces variability \n",
    "- Can introduce bias\n",
    "- Doesn't account for relationships between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe753ec-2572-4e09-8576-8c71fc86631e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imputer Overview\n",
    "\n",
    "An **Imputer** is a tool or class used to fill in missing values in a dataset. It automates the process of replacing missing data with estimated values based on a chosen strategy (e.g., mean, median, mode, or a constant). This ensures that the dataset is complete and suitable for further analysis or modeling.\n",
    "\n",
    "The code below demonstrates how to use an Imputer to handle missing values. It typically involves:\n",
    "1. **Selecting a strategy** (e.g., mean, median, most frequent).\n",
    "2. **Fitting the imputer** to the data to learn the replacement values.\n",
    "3. **Transforming the dataset** by replacing missing values with the learned values.\n",
    "\n",
    "This process helps maintain the integrity of the dataset and allows machine learning algorithms to work without errors due to missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71a8b70e-f73a-4473-8f9c-c1f3fb16be1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Mean Imputation \n",
    "\n",
    "**Mean Imputation** replaces missing values in a numerical feature with the mean (average) of the observed values in that feature. The value used for imputation is calculated from the available (non-missing) data. This approach is simple and quick, but it can reduce variability and may introduce bias if the data is not missing completely at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76d02c29-9f9f-4144-ac32-2b13c3e66456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mean Imputation with Spark ML Imputer\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Select numerical columns for imputation\n",
    "numeric_cols = [\"age\", \"weight\", \"height\", \"blood_pressure\", \"income\"]\n",
    "output_cols = [f\"{col}_imputed\" for col in numeric_cols]\n",
    "\n",
    "# Mean Imputer\n",
    "mean_imputer = Imputer(\n",
    "    inputCols=numeric_cols,\n",
    "    outputCols=output_cols,\n",
    "    strategy=\"mean\"\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "mean_imputer_model = mean_imputer.fit(df_missing)\n",
    "df_mean_imputed = mean_imputer_model.transform(df_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d44ad691-10a1-44a6-bac6-fe39b9706463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_mean_imputed.createOrReplaceTempView(\"mean_imputed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fa501c2-616f-4792-a331-8919465c073d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace table silver_patinet_data_imputed\n",
    "as\n",
    "select \n",
    "patient_id,\n",
    "cast(age_imputed as int) as age,\n",
    "case when age is null then 1 else 0 end age_imputed,\n",
    "cast(weight_imputed as decimal(6,2)) as weight,\n",
    "case when weight is null then 1 else 0 end weight_imputed,\n",
    "cast(height_imputed as decimal(6,2)) as height,\n",
    "case when height is null then 1 else 0 end height_imputed,\n",
    "cast(blood_pressure_imputed as decimal(6,2)) blood_pressure,\n",
    "case when blood_pressure_imputed is null then 1 else 0 end blood_pressure_imputed,\n",
    "cast(income_imputed as decimal(12,2)) as income,\n",
    "case when income is null then 1 else 0 end income_imputed\n",
    "from mean_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90db4b63-dca6-4d16-9af7-dd03ca0d71a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver_imputed = spark.table(\"silver_patinet_data_imputed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba1276d6-4d54-4f32-b5bb-3a77f141cd6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Comparing Base Table with Mean-Imputed Table\n",
    "\n",
    "To evaluate the impact of mean imputation, compare the original table (with missing values) to the table after mean imputation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c3db85-d314-4807-a6b6-3f2595a7daaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_missing.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dceb8c16-51b1-4fb3-b9bb-09510dcb65bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_silver_imputed.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c36e949-2095-47aa-a804-b7a7f70cf72b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Median Imputation\n",
    "\n",
    "**Median Imputation** replaces missing values in a numerical feature with the median of the observed values in that feature. This method is robust to outliers and is often preferred over mean imputation when the data is skewed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0d5d41a-ca3a-4b90-a2a4-4a1a2925fc49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Median Imputation\n",
    "median_imputer = Imputer(\n",
    "    inputCols=numeric_cols,\n",
    "    outputCols=[f\"{col}_median\" for col in numeric_cols],\n",
    "    strategy=\"median\"\n",
    ")\n",
    "\n",
    "median_imputer_model = median_imputer.fit(df_missing)\n",
    "df_median_imputed = median_imputer_model.transform(df_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8999c8e6-d05b-4f91-8d8e-226566f9dab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_median_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c120e65-9713-4e2e-bffb-fa88ba03d349",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Original statistics (without missing values)\n",
    "orig_stats = df_complete.select(\"weight\").describe()\n",
    "print(\"\\nOriginal statistics:\")\n",
    "orig_stats.display()\n",
    "\n",
    "# Statistics after mean imputation\n",
    "mean_stats = df_mean_imputed.select(\"weight_imputed\").describe()\n",
    "print(\"After mean imputation:\")\n",
    "mean_stats.display()\n",
    "\n",
    "# Statistics after median imputation\n",
    "median_stats = df_median_imputed.select(\"weight_median\").describe()\n",
    "print(\"After median imputation:\")\n",
    "median_stats.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd1ac9c2-74b0-4e04-942a-24b07757e26a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Advanced Imputation Methods\n",
    "\n",
    "## Theory\n",
    "\n",
    "**Advanced Imputation** uses relationships between variables to better predict missing values.\n",
    "\n",
    "### \uD83D\uDD04 K-Nearest Neighbors (KNN) Imputation\n",
    "- **Finds K nearest neighbors** for observations with missing values\n",
    "- **Imputes mean/median** from neighbor values\n",
    "- ✅ **Advantages**: Considers local patterns, preserves relationships\n",
    "- ❌ **Disadvantages**: Computationally expensive, sensitive to curse of dimensionality\n",
    "\n",
    "### \uD83D\uDD04 Regression Imputation\n",
    "- **Predicts missing values** using other variables\n",
    "- **Trains regression model** for each variable with missing values\n",
    "- ✅ **Advantages**: Uses all available information\n",
    "- ❌ **Disadvantages**: Can be too precise, ignores uncertainty\n",
    "\n",
    "### \uD83D\uDD04 MICE (Multiple Imputation by Chained Equations)\n",
    "- **Iterative process** - imputes one variable at a time\n",
    "- **Uses all other variables** as predictors\n",
    "- **Multiple imputation** - generates several complete datasets\n",
    "- ✅ **Advantages**: Accounts for uncertainty, very effective\n",
    "- ❌ **Disadvantages**: Complex, time-consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33ef8b1f-0980-4401-bd6a-56a587443666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data preparation for sklearn (KNN, MICE)\n",
    "# Convert to pandas for advanced methods\n",
    "df_pandas = df_missing.select(*numeric_cols).toPandas()\n",
    "\n",
    "print(\"Data preparation for advanced methods:\")\n",
    "print(f\"Shape: {df_pandas.shape}\")\n",
    "print(f\"Missing values per column:\")\n",
    "print(df_pandas.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de58bc65-2602-409a-a9a2-3cc68a0a10ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## KNN Imputation: \n",
    "\n",
    "This section applies K-Nearest Neighbors imputation to fill missing values in the numeric columns of the dataset using sklearn's KNNImputer. It uses 5 nearest neighbors to estimate and replace missing values based on the similarity to other rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32bde0d3-ebc9-4061-8b0b-98f703ec3909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# KNN Imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# KNN Imputer with K=5 neighbors\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df_knn_imputed = knn_imputer.fit_transform(df_pandas)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "df_knn_pandas = pd.DataFrame(df_knn_imputed, columns=numeric_cols)\n",
    "\n",
    "print(\"KNN Imputation (K=5) completed\")\n",
    "print(f\"Missing values after KNN: {df_knn_pandas.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bd7906a-afad-4f69-83bc-05750a8a0280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MICE (Multiple Imputation by Chained Equations)\n",
    "\n",
    "**MICE** is an advanced imputation technique that fills in missing values by modeling each variable with missing data as a function of other variables in a round-robin fashion. It performs multiple rounds of imputation, creating several complete datasets to account for the uncertainty of missing values.\n",
    "\n",
    "### How MICE Works:\n",
    "1. **Initial Imputation:** Fill missing values with simple methods (mean, median, etc.).\n",
    "2. **Iterative Modeling:** For each variable with missing data, regress it on the other variables and update the missing values with predictions.\n",
    "3. **Repeat:** Cycle through all variables multiple times to refine imputations.\n",
    "4. **Multiple Datasets:** Generate several imputed datasets to reflect uncertainty.\n",
    "\n",
    "### Advantages:\n",
    "- Accounts for relationships between variables\n",
    "- Provides more accurate and robust imputations\n",
    "- Quantifies uncertainty by creating multiple datasets\n",
    "\n",
    "### Disadvantages:\n",
    "- Computationally intensive\n",
    "- More complex to implement and interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a5ee454-2620-4f4d-ace6-cafde9e0800a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MICE (Multiple Imputation by Chained Equations)\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# MICE Imputer\n",
    "mice_imputer = IterativeImputer(random_state=42, max_iter=10)\n",
    "df_mice_imputed = mice_imputer.fit_transform(df_pandas)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "df_mice_pandas = pd.DataFrame(df_mice_imputed, columns=numeric_cols)\n",
    "\n",
    "print(\"MICE Imputation completed\")\n",
    "print(f\"Missing values after MICE: {df_mice_pandas.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1b6f66d-74f7-45b8-be20-7d428ba9a916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Comparison of All Methods\n",
    "\n",
    "## Evaluation Framework\n",
    "\n",
    "We'll compare all imputation methods in terms of:\n",
    "- **Distribution preservation** (mean, standard deviation)\n",
    "- **Correlation preservation** between variables\n",
    "- **Bias introduced** by each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b716ed8c-253a-4e1c-a19b-8b3bac09544c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Comparison of all methods - preservation of means\n",
    "print(\"Comparison of means after different imputation methods:\")\n",
    "print(f\"{'Method':<15} {'Age':<8} {'Weight':<8} {'Height':<8} {'BP':<8} {'Income':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Original means (without missing values)\n",
    "orig_means = [df_complete.agg(mean(col)).collect()[0][0] for col in numeric_cols]\n",
    "print(f\"{'Original':<15} {orig_means[0]:<8.1f} {orig_means[1]:<8.1f} {orig_means[2]:<8.1f} {orig_means[3]:<8.1f} {orig_means[4]:<10.0f}\")\n",
    "\n",
    "# Observed (with missing values)\n",
    "obs_means = [df_missing.agg(mean(col)).collect()[0][0] for col in numeric_cols]\n",
    "print(f\"{'Observed':<15} {obs_means[0]:<8.1f} {obs_means[1]:<8.1f} {obs_means[2]:<8.1f} {obs_means[3]:<8.1f} {obs_means[4]:<10.0f}\")\n",
    "\n",
    "# Mean Imputation\n",
    "mean_means = [df_mean_imputed.agg(mean(f\"{col}_imputed\")).collect()[0][0] for col in numeric_cols]\n",
    "print(f\"{'Mean Imputed':<15} {mean_means[0]:<8.1f} {mean_means[1]:<8.1f} {mean_means[2]:<8.1f} {mean_means[3]:<8.1f} {mean_means[4]:<10.0f}\")\n",
    "\n",
    "# KNN\n",
    "knn_means = df_knn_pandas.mean().values\n",
    "print(f\"{'KNN':<15} {knn_means[0]:<8.1f} {knn_means[1]:<8.1f} {knn_means[2]:<8.1f} {knn_means[3]:<8.1f} {knn_means[4]:<10.0f}\")\n",
    "\n",
    "# MICE  \n",
    "mice_means = df_mice_pandas.mean().values\n",
    "print(f\"{'MICE':<15} {mice_means[0]:<8.1f} {mice_means[1]:<8.1f} {mice_means[2]:<8.1f} {mice_means[3]:<8.1f} {mice_means[4]:<10.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac40cfb9-d699-47c2-87c4-55e14aea2d81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Comparison of standard deviations\n",
    "print(\"\\n\uD83D\uDCCA Comparison of standard deviations:\")\n",
    "print(f\"{'Method':<15} {'Age':<8} {'Weight':<8} {'Height':<8} {'BP':<8} {'Income':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Original std\n",
    "orig_stds = [df_complete.agg(stddev(col)).collect()[0][0] for col in numeric_cols]\n",
    "print(f\"{'Original':<15} {orig_stds[0]:<8.1f} {orig_stds[1]:<8.1f} {orig_stds[2]:<8.1f} {orig_stds[3]:<8.1f} {orig_stds[4]:<10.0f}\")\n",
    "\n",
    "# Mean Imputation std\n",
    "mean_stds = [df_mean_imputed.agg(stddev(f\"{col}_imputed\")).collect()[0][0] for col in numeric_cols]\n",
    "print(f\"{'Mean Imputed':<15} {mean_stds[0]:<8.1f} {mean_stds[1]:<8.1f} {mean_stds[2]:<8.1f} {mean_stds[3]:<8.1f} {mean_stds[4]:<10.0f}\")\n",
    "\n",
    "# KNN std\n",
    "knn_stds = df_knn_pandas.std().values\n",
    "print(f\"{'KNN':<15} {knn_stds[0]:<8.1f} {knn_stds[1]:<8.1f} {knn_stds[2]:<8.1f} {knn_stds[3]:<8.1f} {knn_stds[4]:<10.0f}\")\n",
    "\n",
    "# MICE std\n",
    "mice_stds = df_mice_pandas.std().values  \n",
    "print(f\"{'MICE':<15} {mice_stds[0]:<8.1f} {mice_stds[1]:<8.1f} {mice_stds[2]:<8.1f} {mice_stds[3]:<8.1f} {mice_stds[4]:<10.0f}\")\n",
    "\n",
    "print(\"\\n\uD83D\uDCA1 Observations:\")\n",
    "print(\"- Mean imputation often reduces variance\")\n",
    "print(\"- KNN and MICE better preserve distributions\")\n",
    "print(\"- MICE usually closest to original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e025190-bc55-4ca0-871b-c7586ade4814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Decision Framework - Choosing Imputation Method\n",
    "\n",
    "## When to use which method?\n",
    "\n",
    "### 1️⃣ **Deletion Methods**\n",
    "✅ **Use when:**\n",
    "- **Few missing values** (<5%) and MCAR\n",
    "- **Large dataset** - you can afford to lose data\n",
    "- **Complete case analysis** is required\n",
    "\n",
    "### 2️⃣ **Simple Imputation** \n",
    "✅ **Use when:**\n",
    "- **Quick prototyping** - you need a baseline\n",
    "- **Missing data is MCAR** - simple methods suffice\n",
    "- **Large datasets** - advanced methods are expensive\n",
    "\n",
    "### 3️⃣ **KNN Imputation**\n",
    "✅ **Use when:**\n",
    "- **Local patterns** are important\n",
    "- **Mixed data types** (numerical + categorical)\n",
    "- **Medium-sized datasets** (not too large due to cost)\n",
    "\n",
    "### 4️⃣ **MICE**\n",
    "✅ **Use when:**\n",
    "- **MAR/MNAR mechanisms** - you need sophisticated approach\n",
    "- **Research/analysis** - you want highest quality\n",
    "- **Multiple variables** have missing values\n",
    "- **Uncertainty quantification** is important\n",
    "\n",
    "## ⚠️ Key Principles\n",
    "\n",
    "1. **Always analyze missing data mechanism before choosing method**\n",
    "2. **Validate imputation** - compare distributions before/after\n",
    "3. **Don't impute target variable** in supervised learning\n",
    "4. **Consider domain knowledge** - does 0 make sense?\n",
    "5. **Test multiple methods** and choose best for specific problem"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8698289734080733,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Data_Imputing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}