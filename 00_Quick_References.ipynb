{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a10a4ad2-8cf1-48fb-a560-de888bf889d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Data Preparation in ML - Quick Reference\n",
    "## Essential Code Patterns for ML Associate Certification\n",
    "\n",
    "**Part of the Databricks Data Preparation in ML Training Series**\n",
    "\n",
    "---\n",
    "\n",
    "## Must-Know PySpark Patterns\n",
    "\n",
    "This quick reference provides essential code patterns and concepts needed for the Databricks ML Associate certification exam, covering all key data preparation techniques from the training series.\n",
    "\n",
    "### **Data Quality & Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74cfda69-35b1-4195-8541-55684f5cb84c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Missing value analysis\n",
    "from pyspark.sql.functions import col, when, count, isnan, isnull, sum as spark_sum\n",
    "\n",
    "# Count missing values per column\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Completeness percentage\n",
    "total_count = df.count()\n",
    "df.select([(count(c)/total_count*100).alias(f\"{c}_completeness\") for c in df.columns]).show()\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_clean = df.dropna()  # Drop any row with null\n",
    "df_clean = df.dropna(subset=['col1', 'col2'])  # Drop if null in specific columns\n",
    "\n",
    "# Fill missing values\n",
    "df_filled = df.fillna(0)  # Fill all nulls with 0\n",
    "df_filled = df.fillna({'numeric_col': 0, 'string_col': 'unknown'})  # Column-specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0312695-be81-4586-9604-16b419db80e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Imputation with PySpark ML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d37fe89a-fd80-4b29-a45d-06af793a1505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Imputer for numerical columns\n",
    "imputer = Imputer(\n",
    "    inputCols=['age', 'salary', 'score'],\n",
    "    outputCols=['age_imp', 'salary_imp', 'score_imp'],\n",
    "    strategy='mean'  # or 'median', 'mode'\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "imputer_model = imputer.fit(df)\n",
    "df_imputed = imputer_model.transform(df)\n",
    "\n",
    "# Custom imputation with business logic\n",
    "df_custom = df.withColumn(\n",
    "    'age_filled',\n",
    "    when(col('age').isNull(), 35.0).otherwise(col('age'))  # Fill with business default\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f682964-e769-47a4-b512-97701c89e172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3️⃣ **Categorical Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d9f0dca-5f93-415d-801e-8eaa1cd25bfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "# Label/String Indexing (Ordinal)\n",
    "indexer = StringIndexer(inputCol='category', outputCol='category_idx')\n",
    "indexer_model = indexer.fit(df)\n",
    "df_indexed = indexer_model.transform(df)\n",
    "\n",
    "# One-Hot Encoding (Nominal)\n",
    "encoder = OneHotEncoder(inputCols=['category_idx'], outputCols=['category_ohe'])\n",
    "encoder_model = encoder.fit(df_indexed)\n",
    "df_encoded = encoder_model.transform(df_indexed)\n",
    "\n",
    "# Complete pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[indexer, encoder])\n",
    "pipeline_model = pipeline.fit(df)\n",
    "df_final = pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cea903f-8195-4622-b1a3-59b65a2836f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4️⃣ **Feature Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "428df743-368b-493f-a7ae-ff85ba139cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, MinMaxScaler, RobustScaler, VectorAssembler\n",
    "\n",
    "# Prepare features for scaling\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['age', 'salary', 'score'],\n",
    "    outputCol='features'\n",
    ")\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "# StandardScaler (z-score normalization)\n",
    "scaler = StandardScaler(\n",
    "    inputCol='features',\n",
    "    outputCol='scaled_features',\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "# MinMaxScaler (0-1 scaling)\n",
    "minmax_scaler = MinMaxScaler(\n",
    "    inputCol='features',\n",
    "    outputCol='minmax_features'\n",
    ")\n",
    "\n",
    "# RobustScaler (outlier-resistant)\n",
    "robust_scaler = RobustScaler(\n",
    "    inputCol='features',\n",
    "    outputCol='robust_features'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afec426a-a55e-4207-b1ad-61d3f46fa1f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5️⃣ **Data Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af971a1a-1924-4a27-a5b5-62d5d9502d68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Basic train-test split\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Three-way split\n",
    "train_df, temp_df = df.randomSplit([0.6, 0.4], seed=42)\n",
    "val_df, test_df = temp_df.randomSplit([0.5, 0.5], seed=42)\n",
    "\n",
    "# Cross-validation setup\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=model,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=BinaryClassificationEvaluator(),\n",
    "    numFolds=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "cv_model = cv.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66796e0c-1b17-454b-b64f-6def4c39d352",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6️⃣ **Data Validation & Quality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ac760bf-93c3-46cd-bc5c-d3eb360cbe36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev, min as spark_min, max as spark_max, percentile_approx\n",
    "\n",
    "# Basic statistics\n",
    "df.describe().show()\n",
    "\n",
    "# Outlier detection (IQR method)\n",
    "stats = df.select(\n",
    "    percentile_approx('salary', 0.25).alias('q1'),\n",
    "    percentile_approx('salary', 0.75).alias('q3')\n",
    ").collect()[0]\n",
    "\n",
    "iqr = stats['q3'] - stats['q1']\n",
    "lower_bound = stats['q1'] - 1.5 * iqr\n",
    "upper_bound = stats['q3'] + 1.5 * iqr\n",
    "\n",
    "outliers = df.filter(\n",
    "    (col('salary') < lower_bound) | (col('salary') > upper_bound)\n",
    ")\n",
    "\n",
    "# Duplicate detection\n",
    "duplicates = df.groupBy('customer_id').count().filter(col('count') > 1)\n",
    "df_deduped = df.dropDuplicates(['customer_id'])\n",
    "\n",
    "# Data quality checks\n",
    "total_records = df.count()\n",
    "quality_report = df.select(\n",
    "    [((total_records - count(when(col(c).isNull(), c))) / total_records * 100).alias(f\"{c}_completeness\") \n",
    "     for c in df.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91b08fcd-314b-4ca4-be18-dc70f009fdce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDFAF Key Decision Trees\n",
    "\n",
    "### **Missing Data Strategy**\n",
    "```\n",
    "Missing < 5%? → Drop rows\n",
    "Missing 5-15%? → Impute (mean/median/mode)\n",
    "Missing > 15%? → Investigate mechanism (MCAR/MAR/MNAR)\n",
    "                 ↓\n",
    "                Advanced imputation or feature engineering\n",
    "```\n",
    "\n",
    "### **Encoding Strategy**\n",
    "```\n",
    "Categorical Variable?\n",
    "├─ Ordinal → StringIndexer (Label Encoding)\n",
    "└─ Nominal\n",
    "   ├─ Low cardinality (<10) → OneHotEncoder\n",
    "   ├─ High cardinality (>50) → Target Encoding\n",
    "   └─ Medium → Context dependent\n",
    "```\n",
    "\n",
    "### **Scaling Strategy**\n",
    "```\n",
    "Algorithm?\n",
    "├─ Tree-based (RF, XGBoost) → No scaling needed\n",
    "├─ Distance-based (KNN, SVM) → StandardScaler\n",
    "├─ Neural Networks → StandardScaler or MinMaxScaler\n",
    "└─ Linear Models → StandardScaler\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97867795-b82d-499c-9ae9-24d883c2796e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ⚠️ Common Pitfalls\n",
    "\n",
    "### **Data Leakage Prevention**\n",
    "```python\n",
    "# ❌ WRONG - fit on full dataset\n",
    "scaler = StandardScaler()\n",
    "scaler_model = scaler.fit(full_df)  # Data leakage!\n",
    "\n",
    "# ✅ CORRECT - fit only on training\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2])\n",
    "scaler_model = scaler.fit(train_df)  # Fit only on train\n",
    "train_scaled = scaler_model.transform(train_df)\n",
    "test_scaled = scaler_model.transform(test_df)  # Apply same transformation\n",
    "```\n",
    "\n",
    "### **Encoding Mistakes**\n",
    "```python\n",
    "# ❌ WRONG - Label encoding for nominal categories\n",
    "# This creates artificial ordering: Red=0, Blue=1, Green=2\n",
    "colors_indexed = StringIndexer(inputCol='color', outputCol='color_idx')\n",
    "\n",
    "# ✅ CORRECT - One-hot for nominal categories\n",
    "colors_indexed = StringIndexer(inputCol='color', outputCol='color_idx')\n",
    "colors_ohe = OneHotEncoder(inputCols=['color_idx'], outputCols=['color_ohe'])\n",
    "```\n",
    "\n",
    "### **Imputation Errors**\n",
    "```python\n",
    "# ❌ WRONG - Imputing target variable\n",
    "imputer = Imputer(inputCols=['features', 'target'])  # Never impute target!\n",
    "\n",
    "# ✅ CORRECT - Only impute features\n",
    "imputer = Imputer(inputCols=['feature1', 'feature2'])  # Only features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fa7ed35-a270-4f0c-ad56-a42118bed879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83D\uDCCA Exam Quick Facts\n",
    "\n",
    "### **PySpark ML Key Classes**\n",
    "- `StringIndexer` - Categorical → Numeric\n",
    "- `OneHotEncoder` - Nominal categories → Binary vectors\n",
    "- `VectorAssembler` - Multiple columns → Single vector\n",
    "- `StandardScaler` - Z-score normalization\n",
    "- `MinMaxScaler` - 0-1 scaling\n",
    "- `Imputer` - Fill missing values\n",
    "- `CrossValidator` - K-fold validation\n",
    "\n",
    "### **Data Quality Dimensions**\n",
    "- **Completeness** - % non-null values\n",
    "- **Accuracy** - Correctness of values\n",
    "- **Consistency** - Format uniformity\n",
    "- **Validity** - Meets business rules\n",
    "- **Uniqueness** - Appropriate duplicates\n",
    "- **Timeliness** - Currency of data\n",
    "\n",
    "### **Missing Data Mechanisms**\n",
    "- **MCAR** - Missing Completely at Random\n",
    "- **MAR** - Missing at Random (depends on observed)\n",
    "- **MNAR** - Missing Not at Random (depends on missing value)\n",
    "\n",
    "### **Medallion Architecture**\n",
    "- **Bronze** - Raw data with minimal processing\n",
    "- **Silver** - Cleaned, validated data\n",
    "- **Gold** - Business-ready aggregated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9419b68e-abbf-4541-93af-61877cb27986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDFAF Last-Minute Review Checklist\n",
    "\n",
    "### ✅ **Must Remember Code Patterns**\n",
    "- [ ] Missing value counting: `count(when(col(c).isNull(), c))`\n",
    "- [ ] Train-test split: `df.randomSplit([0.8, 0.2], seed=42)`\n",
    "- [ ] String indexing: `StringIndexer(inputCol, outputCol)`\n",
    "- [ ] One-hot encoding: `OneHotEncoder(inputCols, outputCols)`\n",
    "- [ ] Standard scaling: `StandardScaler(inputCol, outputCol, withMean=True)`\n",
    "- [ ] Imputation: `Imputer(inputCols, outputCols, strategy='mean')`\n",
    "- [ ] Outlier detection: `percentile_approx` + IQR calculation\n",
    "\n",
    "### ✅ **Must Know Concepts**\n",
    "- [ ] When to use each encoding method\n",
    "- [ ] When to use each scaling method\n",
    "- [ ] Data leakage prevention\n",
    "- [ ] Missing data mechanism types\n",
    "- [ ] Cross-validation setup\n",
    "- [ ] Business rule validation\n",
    "- [ ] Medallion architecture layers\n",
    "\n",
    "### ✅ **Must Avoid Mistakes**\n",
    "- [ ] Fitting transformers on full dataset before split\n",
    "- [ ] Using label encoding for nominal categories\n",
    "- [ ] Imputing target variables\n",
    "- [ ] Ignoring business context in data decisions\n",
    "- [ ] Scaling tree-based algorithms unnecessarily\n",
    "\n",
    "---\n",
    "\n",
    "## \uD83D\uDE80 **You're Ready! Good Luck on Your Certification!** \uD83C\uDFAF"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_Quick_References",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}