{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76a3450b-ced0-4554-9bb0-3802b9d5e87f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Data Preparation in ML - Notebook 07\n",
    "## Feature Store Practical Demo\n",
    "\n",
    "**Part of the Databricks Data Preparation in ML Training Series**\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Hands-on Feature Store demo in Databricks:\n",
    "\n",
    "- **Feature Store Setup** - Initialize and configure Feature Store\n",
    "- **Feature Tables** - Create and manage feature tables\n",
    "- **Feature Publishing** - Write and update features\n",
    "- **Model Training** - Use features for ML training\n",
    "- **MLflow Integration** - Track features with models\n",
    "\n",
    "## Duration: ~30 minutes | Level: Intermediate\n",
    "\n",
    "---\n",
    "\n",
    "## What is Feature Store?\n",
    "\n",
    "**Centralized feature management platform:**\n",
    "- **Shared Features**: Reuse features across teams\n",
    "- **Point-in-Time**: Historical feature values for training\n",
    "- **Versioning**: Track feature changes over time\n",
    "- **Serving**: Online and batch feature serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65114d51-3cea-4bc0-aa34-0c9ec01921e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Feature Store Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "479acff2-4495-4b1c-824e-ac7cc695ee8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Feature Store\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import mlflow\n",
    "\n",
    "# Create Feature Store client\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "# Setup database\n",
    "DATABASE_NAME = \"feature_store_demo\"\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\")\n",
    "spark.sql(f\"USE {DATABASE_NAME}\")\n",
    "\n",
    "# Verify setup\n",
    "spark.sql(\"SELECT 'Feature Store ready!' as status\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5782a06a-a24a-46ca-8bca-f63f3b95e8e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0624bb1f-550d-444b-a32e-10c01ce95670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create customer features dataset\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Generate 500 customer records (smaller for demo)\n",
    "random.seed(42)\n",
    "customers = []\n",
    "\n",
    "for i in range(1, 501):\n",
    "    customer_id = f\"CUST_{i:04d}\"\n",
    "    feature_timestamp = datetime.now() - timedelta(days=random.randint(0, 30))\n",
    "    \n",
    "    customers.append((\n",
    "        customer_id,\n",
    "        feature_timestamp,\n",
    "        random.randint(1, 50),                    # total_orders\n",
    "        random.uniform(100, 5000),                # total_spent\n",
    "        random.randint(1, 365),                   # days_since_last_order\n",
    "        random.choice(['Electronics', 'Clothing', 'Home', 'Books'])  # favorite_category\n",
    "    ))\n",
    "\n",
    "# Create DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"feature_timestamp\", TimestampType(), False),\n",
    "    StructField(\"total_orders\", IntegerType(), True),\n",
    "    StructField(\"total_spent\", DoubleType(), True),\n",
    "    StructField(\"days_since_last_order\", IntegerType(), True),\n",
    "    StructField(\"favorite_category\", StringType(), True)\n",
    "])\n",
    "\n",
    "customer_features = spark.createDataFrame(customers, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5030da5c-bc23-4d32-86ed-c77c0ef106b7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753560315855}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preview sample data\n",
    "display(customer_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d6f7069-c0ed-4d16-b2cd-b8c979088f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Create Feature Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "130c6c52-9cad-465f-a13c-abf2e97a6ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create feature table in Feature Store\n",
    "feature_table_name = f\"{DATABASE_NAME}.customer_features\"\n",
    "\n",
    "# Create table with primary key and timestamp\n",
    "fs.create_table(\n",
    "    name=feature_table_name,\n",
    "    primary_keys=[\"customer_id\"],\n",
    "    timestamp_keys=[\"feature_timestamp\"],\n",
    "    df=customer_features,\n",
    "    description=\"Customer behavioral features for ML models\"\n",
    ")\n",
    "\n",
    "# Display table info\n",
    "fs.get_table(feature_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2a4c968-c673-4fc5-96ad-a7b3b198fda7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Read Features from Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "292726f8-b34b-4ce2-a3de-bc1ec296408c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read features from Feature Store\n",
    "features_df = fs.read_table(feature_table_name)\n",
    "\n",
    "# Show latest features\n",
    "display(features_df.orderBy(\"customer_id\").limit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d10df7b2-d33d-4238-b520-6f91e0390d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Training with Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3088a2b-5e7b-40ef-a025-a74347bebd4e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753561538119}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Create training labels\n",
    "training_labels = []\n",
    "for i in range(1, 101):\n",
    "    customer_id = f\"CUST_{i:04d}\"\n",
    "    # Simulate high-value customer target\n",
    "    is_high_value = random.choice([0, 1])\n",
    "    training_labels.append((customer_id, is_high_value))\n",
    "\n",
    "tr_label_df = spark.createDataFrame(training_labels, [\"customer_id\", \"target\"])\n",
    "\n",
    "label_df = tr_label_df.join(features_df, on=\"customer_id\")\n",
    "\n",
    "label_df = label_df.select(\"customer_id\",\"feature_timestamp\",\"target\")\n",
    "# Preview sample data\n",
    "display(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9eda1fe-c5d8-4861-a3e6-341c0a5b1466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create training set with Feature Store\n",
    "from databricks.feature_store import FeatureLookup\n",
    "\n",
    "feature_lookups = [\n",
    "    FeatureLookup(\n",
    "        table_name=feature_table_name,\n",
    "        lookup_key=\"customer_id\",\n",
    "        timestamp_lookup_key=\"lookup_timestamp\",\n",
    "        feature_names=[\"total_orders\", \"total_spent\", \"days_since_last_order\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create training set\n",
    "training_set = fs.create_training_set(\n",
    "    df=labels_df,\n",
    "    feature_lookups=feature_lookups,\n",
    "    label=\"target\",\n",
    "    exclude_columns=[\"lookup_timestamp\"]\n",
    ")\n",
    "\n",
    "# Load training data\n",
    "training_df = training_set.load_df()\n",
    "display(training_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d8a4be6-1ba1-4ce1-b88b-6683a4c111b5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753560997063}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3a5d735-50db-4dc7-844d-8869d6efe61d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "from databricks.feature_store import FeatureLookup\n",
    "\n",
    "# Initialize clients\n",
    "fs = FeatureStoreClient()\n",
    "mlflow.set_experiment(\"/Shared/feature_store_ml_experiments\")\n",
    "\n",
    "# Start MLflow run for feature store training\n",
    "with mlflow.start_run(run_name=\"feature_store_training_pipeline\"):\n",
    "    \n",
    "    # 1. Create feature lookups with MLflow tracking\n",
    "    feature_lookups = [\n",
    "        FeatureLookup(\n",
    "            table_name=feature_table_name,\n",
    "            lookup_key=\"customer_id\",\n",
    "            timestamp_lookup_key=\"lookup_timestamp\",\n",
    "            feature_names=[\"total_orders\", \"total_spent\", \"days_since_last_order\"]  \n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Log feature store configuration\n",
    "    mlflow.log_param(\"feature_table\", feature_table_name)\n",
    "    mlflow.log_param(\"feature_count\", len(feature_lookups[0].feature_names))\n",
    "    mlflow.log_param(\"lookup_key\", \"customer_id\")\n",
    "    mlflow.log_param(\"timestamp_aware\", True)\n",
    "    \n",
    "    # 2. Create training set with feature store\n",
    "    print(\" Creating Feature Store training set...\")\n",
    "    training_set = fs.create_training_set(\n",
    "        df=labels_df,\n",
    "        feature_lookups=feature_lookups,\n",
    "        label=\"target\",\n",
    "        exclude_columns=[\"lookup_timestamp\"]\n",
    "    )\n",
    "    \n",
    "    # Load training data\n",
    "    training_df = training_set.load_df()\n",
    "    training_df = training_df.dropna()\n",
    "    training_count = training_df.count()\n",
    "    feature_columns = [col for col in training_df.columns if col not in [\"customer_id\", \"target\"]]\n",
    "    \n",
    "    mlflow.log_metric(\"training_records\", training_count)\n",
    "    mlflow.log_metric(\"feature_count_final\", len(feature_columns))\n",
    "    \n",
    "    # 3. Train a simple model (for demonstration)\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "    \n",
    "    # Prepare features\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    training_df_features = assembler.transform(training_df)\n",
    "    \n",
    "    # Split for validation\n",
    "    train_split, val_split = training_df_features.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # Train model\n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"target\")\n",
    "    model = lr.fit(train_split)\n",
    "    \n",
    "    # Evaluate model\n",
    "    predictions = model.transform(val_split)\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"target\")\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    \n",
    "    mlflow.log_metric(\"validation_auc\", auc)\n",
    "    \n",
    "    # 4. Log model with feature store dependencies\n",
    "    print(\"\uD83D\uDCDD Logging model with Feature Store dependencies...\")\n",
    "    fs.log_model(\n",
    "        model,\n",
    "        \"feature_store_model\",\n",
    "        flavor=mlflow.spark,\n",
    "        training_set=training_set,\n",
    "        registered_model_name=\"customer_prediction_with_features\"\n",
    "    )\n",
    "    \n",
    "    print(f\" Model trained and logged with AUC: {auc:.3f}\")\n",
    "    print(f\" Used {len(feature_columns)} features from Feature Store\")\n",
    "    print(f\" Training set: {training_count} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "771910d4-a147-4918-824c-4b7aa1848c14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### What we accomplished:\n",
    "✅ **Feature Store Setup**: Initialized Feature Store client  \n",
    "✅ **Feature Table**: Created table with primary keys and timestamps  \n",
    "✅ **Data Management**: Stored and retrieved customer features  \n",
    "✅ **Training Integration**: Used features for ML model training  \n",
    "✅ **MLflow Integration**: Tracked models with feature dependencies  \n",
    "\n",
    "### Key Benefits:\n",
    "- **Feature Reuse**: Share features across teams and projects\n",
    "- **Point-in-Time**: Historical feature values for training\n",
    "- **Consistency**: Same features for training and serving\n",
    "- **Governance**: Centralized feature management\n",
    "\n",
    "### Next Steps:\n",
    "1. **Feature Updates**: Implement regular feature refresh pipelines\n",
    "2. **Online Serving**: Set up real-time feature serving\n",
    "3. **Monitoring**: Add feature drift and quality monitoring\n",
    "4. **Production**: Deploy models with Feature Store integration\n",
    "\n",
    "**Duration**: ~30 minutes | **Level**: Intermediate"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_Feature_Store",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}