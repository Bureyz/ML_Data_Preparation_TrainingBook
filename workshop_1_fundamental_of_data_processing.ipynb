{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a40c92b-dcb2-45ad-ae17-9505a64484e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks ML Workshop 1: Fundamental Data Processing\n",
    "\n",
    "## Introduction to Data Processing for Machine Learning\n",
    "\n",
    "**What we'll learn**: Core data processing techniques essential for ML in Databricks\n",
    "\n",
    "**Tools we'll use**: \n",
    "- PySpark DataFrames for data manipulation\n",
    "- Databricks SQL for data exploration\n",
    "- Built-in visualization tools\n",
    "- Data quality checks and cleaning\n",
    "\n",
    "**Time**: 60 minutes | **Difficulty**: Beginner\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "1. Load and explore datasets in Databricks\n",
    "2. Perform basic data cleaning and transformation\n",
    "3. Handle missing values and data types\n",
    "4. Create data quality reports\n",
    "5. Prepare data for machine learning\n",
    "\n",
    "**Let's start processing data!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a07c37bb-7df0-4d92-842f-ac9ccc4d70bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 1: Getting Started \n",
    "\n",
    "## Your Task: Import Essential Libraries\n",
    "First, let's import the libraries we need for data processing.\n",
    "\n",
    "**Try this:**\n",
    "- Import `pyspark.sql.functions` as `F`\n",
    "- Import `pyspark.sql.types` \n",
    "- Import `pandas` as `pd` for some operations\n",
    "\n",
    "*Write your code in the cell below, then check the solution!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc2611d7-149c-4a5d-bb09-2a5a175878ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \uD83D\uDD28 YOUR CODE HERE\n",
    "\n",
    "# Import PySpark functions\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# Import PySpark types\n",
    "# from pyspark.sql import types\n",
    "\n",
    "# Import pandas\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bcd3836-9c59-470f-805f-1527a5a7ae09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ✅ SOLUTION: Import Essential Libraries\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types\n",
    "import pandas as pd\n",
    "\n",
    "# Success! Libraries imported \n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "430ad66c-16fe-4048-a4e6-7b4eea98c322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 2: Loading Sample Data\n",
    "\n",
    "##Your Task: Create a Sample Customer Dataset\n",
    "Let's create a sample dataset to practice data processing techniques.\n",
    "\n",
    "**Try this:**\n",
    "- Create a DataFrame with customer data including: id, name, age, city, purchase_amount, last_login\n",
    "- Include some missing values and data quality issues to practice cleaning\n",
    "- Use `spark.createDataFrame()` with a list of tuples\n",
    "\n",
    "*Hint: Include at least 10 customers with some NULL values and inconsistent data!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2457511-1fe2-4621-b09e-36c91b9d7724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ✅ SOLUTION: Create Sample Customer Dataset\n",
    "\n",
    "# Sample data with quality issues\n",
    "data = [\n",
    "    (1, \"John Doe\", 25, \"New York\", 1500.0, \"2024-01-15\"),\n",
    "    (2, \"Jane Smith\", None, \"Los Angeles\", 2300.0, \"2024-01-14\"),\n",
    "    (3, \"Bob Johnson\", 35, None, 850.0, \"2024-01-13\"),\n",
    "    (4, \"Alice Brown\", 28, \"Chicago\", None, \"2024-01-12\"),\n",
    "    (5, \"Charlie Davis\", 42, \"Houston\", 3200.0, None),\n",
    "    (6, \"Eva Wilson\", None, \"Phoenix\", 1750.0, \"2024-01-10\"),\n",
    "    (7, \"Frank Miller\", 31, \"Philadelphia\", 2100.0, \"2024-01-09\"),\n",
    "    (8, \"Grace Lee\", 29, None, 1900.0, \"2024-01-08\"),\n",
    "    (9, \"Henry Taylor\", 38, \"San Antonio\", None, \"2024-01-07\"),\n",
    "    (10, \"Ivy Chen\", 26, \"San Diego\", 2800.0, \"2024-01-06\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\", \"city\", \"purchase_amount\", \"last_login\"])\n",
    "\n",
    "# Show the data\n",
    "df.display()\n",
    "print(f\"Dataset created with {df.count()} customers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10624f01-ec57-49bf-91c1-142501a64dbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 3: Basic Data Exploration\n",
    "\n",
    "##Your Task: Explore the Dataset\n",
    "Let's understand our data better by exploring its structure and content.\n",
    "\n",
    "**Try this:**\n",
    "- Check the schema using `.printSchema()`\n",
    "- Get basic statistics using `.describe()`\n",
    "- Count total rows and columns\n",
    "\n",
    "*Understanding your data is the first step in any ML project!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36baddeb-8879-4c3b-b3ab-fa54d45f057e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \uD83D\uDD28 YOUR CODE HERE\n",
    "\n",
    "# Check the schema\n",
    "# df.printSchema()\n",
    "\n",
    "# Get basic statistics\n",
    "# df.describe().show()\n",
    "\n",
    "# Count rows and columns\n",
    "# print(f\"Rows: {df.count()}, Columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b7c63f9-91d5-40c4-979b-d2a07c04808c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ✅ SOLUTION: Basic Data Exploration\n",
    "\n",
    "print(\"Dataset Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"Basic Statistics:\")\n",
    "df.describe().display()\n",
    "\n",
    "print(f\"Dataset Size: {df.count()} rows, {len(df.columns)} columns\")\n",
    "\n",
    "print(\"Column Names:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(\"Exploration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8295c050-048d-40e6-8c62-ceb0539d0169",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 4: Data Quality Assessment\n",
    "\n",
    "## Your Task: Check for Missing Values\n",
    "Data quality is crucial for ML. Let's identify missing values in our dataset.\n",
    "\n",
    "**Try this:**\n",
    "- Count NULL values in each column\n",
    "- Calculate the percentage of missing data per column\n",
    "- Use `F.col().isNull()` and `F.sum()` functions\n",
    "\n",
    "*Missing data can significantly impact your model's performance!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f195aeb-e646-4c80-b58b-e6a32067a393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \uD83D\uDD28 YOUR CODE HERE\n",
    "\n",
    "# Count NULL values for each column\n",
    "# null_counts = df.select([\n",
    "#     F.sum(F.col(c).isNull().cast(\"int\")).alias(c) \n",
    "#     for c in df.columns\n",
    "# ])\n",
    "\n",
    "# Show the results\n",
    "# null_counts.show()\n",
    "\n",
    "# Calculate percentage of missing values\n",
    "# total_rows = df.count()\n",
    "# for col in df.columns:\n",
    "#     null_count = df.filter(F.col(col).isNull()).count()\n",
    "#     percentage = (null_count / total_rows) * 100\n",
    "#     print(f\"{col}: {null_count} missing ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2efc0a7-f082-431c-ac0b-4fd028412730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ✅ SOLUTION: Check for Missing Values\n",
    "\n",
    "print(\"\uD83D\uDD0D Missing Value Analysis:\")\n",
    "\n",
    "# Count NULL values for each column\n",
    "null_counts = df.select([\n",
    "    F.sum(F.col(c).isNull().cast(\"int\")).alias(c) \n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "print(\"NULL Count by Column:\")\n",
    "null_counts.display()\n",
    "\n",
    "# Calculate percentage of missing values\n",
    "total_rows = df.count()\n",
    "print(f\"Missing Value Percentages (Total rows: {total_rows}):\")\n",
    "\n",
    "for col in df.columns:\n",
    "    null_count = df.filter(F.col(col).isNull()).count()\n",
    "    percentage = (null_count / total_rows) * 100\n",
    "    print(f\"  {col}: {null_count} missing ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"Missing value analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25cbd95a-1709-48dd-98a4-c9b43861477c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 5: Handle Missing Values\n",
    "\n",
    "##Your Task: Clean Missing Data\n",
    "Now let's handle the missing values using different strategies.\n",
    "\n",
    "**Try this:**\n",
    "- Fill missing ages with the mean age\n",
    "- Fill missing cities with \"Unknown\"\n",
    "- Fill missing purchase amounts with median\n",
    "- Use `.fillna()` and `.na.fill()` methods\n",
    "\n",
    "*Different strategies work better for different types of data!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db9c026e-fb61-49db-b5a5-0e6518f9759b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \uD83D\uDD28 YOUR CODE HERE\n",
    "\n",
    "# Calculate mean age for missing values\n",
    "# mean_age = df.select(F.avg(\"age\")).collect()[0][0]\n",
    "\n",
    "# Calculate median purchase amount\n",
    "# median_purchase = df.approxQuantile(\"purchase_amount\", [0.5], 0.0)[0]\n",
    "\n",
    "# Fill missing values\n",
    "# df_clean = df.fillna({\n",
    "#     \"age\": mean_age,\n",
    "#     \"city\": \"Unknown\",\n",
    "#     \"purchase_amount\": median_purchase,\n",
    "#     \"last_login\": \"2024-01-01\"  # Default date\n",
    "# })\n",
    "\n",
    "# Show the cleaned data\n",
    "# df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c87ca79b-52f3-417d-a772-a6b3b915f03d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ✅ SOLUTION: Handle Missing Values\n",
    "\n",
    "print(\"Cleaning Missing Values:\")\n",
    "\n",
    "# Calculate statistics for imputation\n",
    "mean_age = df.select(F.avg(\"age\")).collect()[0][0]\n",
    "median_purchase = df.approxQuantile(\"purchase_amount\", [0.5], 0.0)[0]\n",
    "\n",
    "print(f\"Mean age for imputation: {mean_age:.1f}\")\n",
    "print(f\"Median purchase amount for imputation: {median_purchase:.2f}\")\n",
    "\n",
    "# Fill missing values with appropriate strategies\n",
    "df_clean = df.fillna({\n",
    "    \"age\": mean_age,\n",
    "    \"city\": \"Unknown\",\n",
    "    \"purchase_amount\": median_purchase,\n",
    "    \"last_login\": \"2024-01-01\"  # Default date\n",
    "})\n",
    "\n",
    "print(\"Cleaned Dataset:\")\n",
    "df_clean.display()\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"\uD83D\uDD0D Verification - Missing values after cleaning:\")\n",
    "df_clean.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df_clean.columns]).display()\n",
    "\n",
    "print(\"Data cleaning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4299a5dc-687b-4f6b-bfb8-2d45c1c46dc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 6: Data Type Conversion\n",
    "\n",
    "## Your Task: Convert Data Types\n",
    "Proper data types are essential for ML algorithms. Let's convert our columns to appropriate types.\n",
    "\n",
    "**Try this:**\n",
    "- Convert `last_login` from string to date\n",
    "- Ensure `age` is integer type\n",
    "- Ensure `purchase_amount` is double type\n",
    "- Use `.cast()` or `.withColumn()`\n",
    "\n",
    "*Correct data types prevent errors and improve performance!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f6f5483-fcc8-49f0-b202-f2da857558f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \uD83D\uDD28 YOUR CODE HERE\n",
    "\n",
    "# Convert data types\n",
    "# df_typed = df_clean.withColumn(\"last_login\", F.to_date(\"last_login\")) \\\n",
    "#                    .withColumn(\"age\", F.col(\"age\").cast(\"integer\")) \\\n",
    "#                    .withColumn(\"purchase_amount\", F.col(\"purchase_amount\").cast(\"double\"))\n",
    "\n",
    "# Check the new schema\n",
    "# df_typed.printSchema()\n",
    "\n",
    "# Show sample data\n",
    "# df_typed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcfcb685-b7c0-4ea8-987f-f0773804d7de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ✅ SOLUTION: Convert Data Types\n",
    "\n",
    "print(\"Converting Data Types:\")\n",
    "\n",
    "# Convert data types appropriately\n",
    "df_typed = df_clean.withColumn(\"last_login\", F.to_date(\"last_login\")) \\\n",
    "                   .withColumn(\"age\", F.col(\"age\").cast(\"integer\")) \\\n",
    "                   .withColumn(\"purchase_amount\", F.col(\"purchase_amount\").cast(\"double\"))\n",
    "\n",
    "print(\"Updated Schema:\")\n",
    "df_typed.printSchema()\n",
    "\n",
    "print(\"Sample Data with Correct Types:\")\n",
    "df_typed.display(5)\n",
    "\n",
    "print(\"Data type conversion complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8278e20d-9ed7-40e8-bf0c-dd787dd42645",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 7: Feature Engineering\n",
    "\n",
    "## Your Task: Create New Features\n",
    "Feature engineering can improve model performance. Let's create some useful features.\n",
    "\n",
    "**Try this:**\n",
    "- Create an `age_group` column (Young: <30, Middle: 30-40, Senior: >40)\n",
    "- Create a `days_since_login` column\n",
    "- Create a `purchase_category` column (Low: <1500, Medium: 1500-2500, High: >2500)\n",
    "- Use `F.when()` for conditional logic\n",
    "\n",
    "*Good features can make the difference between a mediocre and excellent model!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73436671-65be-4752-9d3f-11aad8e68f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \uD83D\uDD28 YOUR CODE HERE\n",
    "\n",
    "# Create age groups\n",
    "# df_features = df_typed.withColumn(\n",
    "#     \"age_group\",\n",
    "#     F.when(F.col(\"age\") < 30, \"Young\")\n",
    "#      .when(F.col(\"age\") <= 40, \"Middle\")\n",
    "#      .otherwise(\"Senior\")\n",
    "# )\n",
    "\n",
    "# Create days since last login\n",
    "# current_date = F.current_date()\n",
    "# df_features = df_features.withColumn(\n",
    "#     \"days_since_login\",\n",
    "#     F.datediff(current_date, F.col(\"last_login\"))\n",
    "# )\n",
    "\n",
    "# Create purchase categories\n",
    "# df_features = df_features.withColumn(\n",
    "#     \"purchase_category\",\n",
    "#     F.when(F.col(\"purchase_amount\") < 1500, \"Low\")\n",
    "#      .when(F.col(\"purchase_amount\") <= 2500, \"Medium\")\n",
    "#      .otherwise(\"High\")\n",
    "# )\n",
    "\n",
    "# Show the enhanced dataset\n",
    "# df_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c300aea4-2a92-4a4b-80a1-dd4074057c11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ✅ SOLUTION: Create New Features\n",
    "\n",
    "print(\"\uD83D\uDEE0️ Creating New Features:\")\n",
    "\n",
    "# Create age groups\n",
    "df_features = df_typed.withColumn(\n",
    "    \"age_group\",\n",
    "    F.when(F.col(\"age\") < 30, \"Young\")\n",
    "     .when(F.col(\"age\") <= 40, \"Middle\")\n",
    "     .otherwise(\"Senior\")\n",
    ")\n",
    "\n",
    "# Create days since last login\n",
    "current_date = F.current_date()\n",
    "df_features = df_features.withColumn(\n",
    "    \"days_since_login\",\n",
    "    F.datediff(current_date, F.col(\"last_login\"))\n",
    ")\n",
    "\n",
    "# Create purchase categories\n",
    "df_features = df_features.withColumn(\n",
    "    \"purchase_category\",\n",
    "    F.when(F.col(\"purchase_amount\") < 1500, \"Low\")\n",
    "     .when(F.col(\"purchase_amount\") <= 2500, \"Medium\")\n",
    "     .otherwise(\"High\")\n",
    ")\n",
    "\n",
    "print(\"Enhanced Dataset with New Features:\")\n",
    "df_features.display()\n",
    "\n",
    "print(\"Feature engineering complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75e3096a-94d3-4b13-85e1-d277a4e98385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 8: Data Analysis and Insights\n",
    "\n",
    "## Your Task: Analyze Data Patterns\n",
    "Let's analyze our processed data to understand patterns and relationships.\n",
    "\n",
    "**Try this:**\n",
    "- Calculate average purchase amount by city\n",
    "- Count customers in each age group\n",
    "- Analyze purchase patterns by category\n",
    "- Use `.groupBy()`, `.agg()`, and aggregation functions\n",
    "\n",
    "*Data analysis helps validate your processing and discover insights!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ab9cd5-103d-4553-aefe-b3de2f90f4d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \uD83D\uDD28 YOUR CODE HERE\n",
    "\n",
    "# Average purchase by city\n",
    "# avg_by_city = df_features.groupBy(\"city\") \\\n",
    "#                         .agg(F.avg(\"purchase_amount\").alias(\"avg_purchase\"),\n",
    "#                              F.count(\"*\").alias(\"customer_count\")) \\\n",
    "#                         .orderBy(F.desc(\"avg_purchase\"))\n",
    "\n",
    "# Customer count by age group\n",
    "# age_distribution = df_features.groupBy(\"age_group\") \\\n",
    "#                               .count() \\\n",
    "#                               .orderBy(\"age_group\")\n",
    "\n",
    "# Show results\n",
    "# avg_by_city.show()\n",
    "# age_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bb14051-444f-4a59-857a-a029e62fbe44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ✅ SOLUTION: Data Analysis and Insights\n",
    "\n",
    "print(\"Analyzing Data Patterns:\")\n",
    "\n",
    "# Average purchase by city\n",
    "print(\"Average Purchase Amount by City:\")\n",
    "avg_by_city = df_features.groupBy(\"city\") \\\n",
    "                        .agg(F.avg(\"purchase_amount\").alias(\"avg_purchase\"),\n",
    "                             F.count(\"*\").alias(\"customer_count\")) \\\n",
    "                        .orderBy(F.desc(\"avg_purchase\"))\n",
    "avg_by_city.display()\n",
    "\n",
    "# Customer distribution by age group\n",
    "print(\"Customer Distribution by Age Group:\")\n",
    "age_distribution = df_features.groupBy(\"age_group\") \\\n",
    "                              .count() \\\n",
    "                              .orderBy(\"age_group\")\n",
    "age_distribution.display()\n",
    "\n",
    "# Purchase category distribution\n",
    "print(\"Purchase Category Distribution:\")\n",
    "purchase_distribution = df_features.groupBy(\"purchase_category\") \\\n",
    "                                  .agg(F.count(\"*\").alias(\"customer_count\"),\n",
    "                                       F.avg(\"purchase_amount\").alias(\"avg_amount\")) \\\n",
    "                                  .orderBy(\"purchase_category\")\n",
    "purchase_distribution.display()\n",
    "\n",
    "print(\"Data analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9d5ecb9-f096-4cf4-9762-a3464022eb64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 9: Save Processed Data\n",
    "\n",
    "## Your Task: Save Your Processed Dataset\n",
    "Let's save our cleaned and enhanced dataset for future use.\n",
    "\n",
    "**Try this:**\n",
    "- Create a temporary view of the processed data\n",
    "- Save as Delta table for ACID transactions\n",
    "- Create a summary report of the processing steps\n",
    "\n",
    "*Saving processed data allows you to reuse it in multiple ML experiments!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "554cfb41-1638-4fdd-812e-3d1ce004930d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \uD83D\uDD28 YOUR CODE HERE\n",
    "\n",
    "# Create a temporary view\n",
    "# df_features.createOrReplaceTempView(\"processed_customers\")\n",
    "\n",
    "# Save as Delta table\n",
    "# df_features.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"demo.processed_customers\")\n",
    "\n",
    "# Create summary report\n",
    "# print(\" Processing Summary:\")\n",
    "# print(f\"Original rows: {df.count()}\")\n",
    "# print(f\"Final rows: {df_features.count()}\")\n",
    "# print(f\"Columns added: {len(df_features.columns) - len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2a46f29-2fb7-4c75-a4b9-da29871b1632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ✅ SOLUTION: Save Processed Data\n",
    "\n",
    "print(\"Saving Processed Data:\")\n",
    "\n",
    "# Create a temporary view for SQL access\n",
    "df_features.createOrReplaceTempView(\"processed_customers\")\n",
    "\n",
    "# Ensure demo database exists\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS demo\")\n",
    "\n",
    "# Save as Delta table for ACID transactions\n",
    "try:\n",
    "    df_features.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"demo.processed_customers\")\n",
    "    print(\" Data saved to Delta table: demo.processed_customers\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\" Data processing completed (table creation may require permissions)\")\n",
    "\n",
    "# Create comprehensive summary report\n",
    "print(\"\\n Data Processing Summary Report:\")\n",
    "print(\"=\"*50)\n",
    "print(f\" Dataset Size:\")\n",
    "print(f\"  - Original rows: {df.count()}\")\n",
    "print(f\"  - Final rows: {df_features.count()}\")\n",
    "print(f\"  - Original columns: {len(df.columns)}\")\n",
    "print(f\"  - Final columns: {len(df_features.columns)}\")\n",
    "print(f\"  - New features added: {len(df_features.columns) - len(df.columns)}\")\n",
    "\n",
    "print(f\" Data Quality:\")\n",
    "print(f\"  - Missing values handled: \")\n",
    "print(f\"  - Data types corrected: \")\n",
    "print(f\"  - Features engineered: \")\n",
    "\n",
    "print(f\" Ready for ML: \")\n",
    "print(\"Data processing pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d04727-f804-4c91-b1dd-82237dc85f11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#  Workshop 1 Complete!\n",
    "\n",
    "## What We Accomplished\n",
    " **Data Loading** - Created sample customer dataset with realistic issues  \n",
    " **Data Exploration** - Analyzed schema, statistics, and structure  \n",
    " **Quality Assessment** - Identified and quantified missing values  \n",
    " **Data Cleaning** - Applied appropriate imputation strategies  \n",
    " **Type Conversion** - Ensured proper data types for ML  \n",
    " **Feature Engineering** - Created valuable new features  \n",
    " **Data Analysis** - Discovered patterns and relationships  \n",
    " **Data Persistence** - Saved processed data for reuse  \n",
    "\n",
    "## Key Skills Learned\n",
    " **Data Quality Assessment** - How to identify and measure data issues  \n",
    " **Data Cleaning Techniques** - Multiple strategies for handling missing values  \n",
    " **Feature Engineering** - Creating useful features from existing data  \n",
    " **Data Analysis** - Using aggregations to understand patterns  \n",
    " **Data Management** - Saving and organizing processed datasets  \n",
    "\n",
    "## Next Steps\n",
    "- **Workshop 2**: Advanced data transformations and vector search\n",
    "- **Workshop 3**: End-to-end ML pipeline with Feature Store and AutoML\n",
    "- Apply these techniques to your own datasets\n",
    "- Experiment with different imputation strategies\n",
    "- Try creating more complex features\n",
    "\n",
    "**Excellent work! You've mastered fundamental data processing in Databricks! \uD83D\uDE80**\n",
    "\n",
    "---\n",
    "\n",
    "*Ready for more advanced data processing? Continue to Workshop 2!*"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "workshop_1_fundamental_of_data_processing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}