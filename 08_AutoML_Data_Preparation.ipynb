{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672630d3-196b-4247-b97e-c07e1b20be45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Data Preparation in ML - Notebook 08\n",
    "## AutoML & MLflow Integration Demo\n",
    "\n",
    "**Part of the Databricks Data Preparation in ML Training Series**\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This hands-on demo shows practical AutoML and MLflow usage in Databricks:\n",
    "\n",
    "- **Simple Data Setup** - Create clean demo dataset\n",
    "- **AutoML Experiment** - Run classification with AutoML\n",
    "- **MLflow Tracking** - Track experiments and models\n",
    "- **Model Registry** - Register models for production\n",
    "- **Model Serving** - Deploy models for predictions\n",
    "\n",
    "## Duration: ~25 minutes | Level: Beginner → Intermediate\n",
    "\n",
    "---\n",
    "\n",
    "## Why AutoML + MLflow?\n",
    "\n",
    "**Perfect combination for rapid ML development:**\n",
    "- **AutoML**: Automatic feature engineering and model selection\n",
    "- **MLflow**: Experiment tracking and model management\n",
    "- **Databricks**: Unified platform for end-to-end ML workflows\n",
    "- **Focus on Business**: Less time on code, more time on insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c71f06f-c18b-4d65-983e-e7908a278da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# AutoML Quick Overview\n",
    "\n",
    "AutoML automates the entire machine learning workflow:\n",
    "\n",
    "**\uD83D\uDD27 Data Preparation:**\n",
    "- Missing value imputation\n",
    "- Categorical encoding\n",
    "- Feature scaling and engineering\n",
    "\n",
    "**\uD83E\uDD16 Model Building:**\n",
    "- Algorithm selection (XGBoost, Random Forest, etc.)\n",
    "- Hyperparameter optimization\n",
    "- Cross-validation\n",
    "\n",
    "**\uD83D\uDCCA Results:**\n",
    "- Performance metrics\n",
    "- Feature importance\n",
    "- Generated code notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0a97875-65ba-4fdc-8e39-14cdb0ff333a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7e2a02c-036b-4507-b9d8-22d17e34627f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "from databricks import automl\n",
    "import mlflow\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc069ac4-99b4-43e0-a51c-bbdcf1565d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Sample Dataset\n",
    "\n",
    "Create a customer dataset with missing values to demonstrate AutoML's data preparation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13cf86f7-edf3-4df6-8f3d-89c9a9debed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Ustaw ziarno dla powtarzalności\n",
    "np.random.seed(42)\n",
    "\n",
    "# Liczba rekordów\n",
    "n = 1000\n",
    "\n",
    "# Generowanie danych jako zwykłe Pythonowe typy (int, str)\n",
    "customer_data_clean = []\n",
    "for i in range(n):\n",
    "    customer_data_clean.append((\n",
    "        f\"CUST_{i:04d}\",\n",
    "        int(np.random.randint(18, 80)),\n",
    "        str(np.random.choice(['Male', 'Female'])),\n",
    "        int(np.random.randint(25000, 120000)),\n",
    "        str(np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'])),\n",
    "        str(np.random.choice(['Warsaw', 'Krakow', 'Gdansk', 'Wroclaw', 'Other'])),\n",
    "        int(np.random.randint(1, 20)),\n",
    "        int(np.random.randint(50, 500)),\n",
    "        int(np.random.randint(1, 60)),\n",
    "        int(np.random.choice([0, 1]))\n",
    "    ))\n",
    "\n",
    "# Schemat\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"income\", IntegerType(), True),\n",
    "    StructField(\"education\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"monthly_purchases\", IntegerType(), True),\n",
    "    StructField(\"avg_purchase_amount\", IntegerType(), True),\n",
    "    StructField(\"tenure_months\", IntegerType(), True),\n",
    "    StructField(\"is_high_value\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Utwórz DataFrame w Spark\n",
    "df_customers = spark.createDataFrame(customer_data_clean, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a901611-53ec-4149-87d6-fa5d57fddcfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Quick overview of our customer dataset\n",
    "df_customers.display(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f91307c-7cf0-4279-8bcd-07b9acb26300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. AutoML Setup\n",
    "\n",
    "Now let's configure and run Databricks AutoML on our customer dataset. AutoML will:\n",
    "- Automatically handle missing values\n",
    "- Select and engineer features\n",
    "- Try multiple ML algorithms\n",
    "- Optimize hyperparameters\n",
    "- Provide interpretability insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d885b8a-48a5-496c-bc83-31f5f9d8798a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run AutoML classification experiment\n",
    "summary = automl.classify(\n",
    "    dataset=df_customers,\n",
    "    target_col=\"is_high_value\",\n",
    "    primary_metric=\"f1\",\n",
    "    timeout_minutes=10,  # Short timeout for demo\n",
    "    max_trials=5,       # Limited trials for demo\n",
    "    experiment_name=\"customer_value_automl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9799669-177a-4f36-b955-2c06c83d33f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access AutoML experiment results\n",
    "experiment_id = summary.experiment.experiment_id\n",
    "best_trial = summary.best_trial\n",
    "\n",
    "# Display basic results\n",
    "experiment_id, best_trial.mlflow_run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25deacca-5537-4c9e-ba89-8a775f3a6d8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. MLflow Experiment Tracking\n",
    "\n",
    "AutoML automatically creates MLflow experiments to track all models and metrics. Let's explore the experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e02317-c7af-4716-9a67-ad625646d907",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Browse experiment runs in MLflow\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "# Display top performing models\n",
    "top_runs = runs.sort_values('metrics.val_f1_score', ascending=False).head(3)\n",
    "display(top_runs[['run_id', 'metrics.val_f1_score', 'metrics.val_precision_score', 'metrics.val_recall_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e8bc5bb-1e8f-4c83-a113-b4ecd4fae10a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load best model for prediction\n",
    "best_model_uri = f\"runs:/{best_trial.mlflow_run_id}/model\"\n",
    "model = mlflow.sklearn.load_model(best_model_uri)\n",
    "\n",
    "# Show sample predictions\n",
    "sample_data = df_customers.select([col for col in df_customers.columns if col != 'is_high_value']).limit(5).toPandas()\n",
    "predictions = model.predict(sample_data)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b6efe7-33b0-45da-8c79-4d1eaff93f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Model Registry\n",
    "\n",
    "Register the best model to MLflow Model Registry for production deployment and versioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9850c7bd-35dd-4929-8415-d660521a8521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the best model to Unity Catalog\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Infer model signature\n",
    "sample_data = df_customers.select([col for col in df_customers.columns if col != 'is_high_value']).limit(100).toPandas()\n",
    "signature = infer_signature(sample_data, model.predict(sample_data))\n",
    "\n",
    "# Register model\n",
    "model_name = \"customer_value_classifier\"\n",
    "registered_model = mlflow.register_model(\n",
    "    model_uri=best_model_uri,\n",
    "    name=model_name,\n",
    "    signature=signature\n",
    ")\n",
    "\n",
    "registered_model.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7679133-c015-4882-8974-6647570c2720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load model from registry for serving\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Get latest version of the model\n",
    "latest_version = client.get_latest_versions(model_name, stages=[\"None\"])[0]\n",
    "\n",
    "model_version_uri = f\"models:/{model_name}/{latest_version.version}\"\n",
    "serving_model = mlflow.sklearn.load_model(model_version_uri)\n",
    "\n",
    "# Test prediction with the registered model\n",
    "test_prediction = serving_model.predict(sample_data.head(1))\n",
    "test_prediction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85adca28-63ea-4b38-bc67-41e62e9120f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Summary and Next Steps\n",
    "\n",
    "### What we accomplished:\n",
    "✅ **Automated Data Preparation**: AutoML handled missing values and feature engineering automatically  \n",
    "✅ **Model Training**: Multiple algorithms tested and optimized automatically  \n",
    "✅ **MLflow Tracking**: All experiments, metrics, and models tracked automatically  \n",
    "✅ **Model Registry**: Best model registered for production use  \n",
    "✅ **Model Serving**: Model ready for batch or real-time predictions  \n",
    "\n",
    "### Next Steps:\n",
    "1. **Monitor Model Performance**: Set up model monitoring in production\n",
    "2. **A/B Testing**: Test model performance against existing solutions\n",
    "3. **Feature Store**: Move to centralized feature management\n",
    "4. **Model Serving**: Deploy model as REST API endpoint\n",
    "5. **Continuous Training**: Set up automated retraining pipelines\n",
    "\n",
    "### Databricks AutoML Benefits:\n",
    "- **No Code Required**: Fully automated ML pipeline\n",
    "- **Best Practices**: Follows ML engineering best practices\n",
    "- **Transparency**: All code and notebooks generated for review\n",
    "- **Integration**: Native MLflow and Unity Catalog integration"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "08_AutoML_Data_Preparation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}