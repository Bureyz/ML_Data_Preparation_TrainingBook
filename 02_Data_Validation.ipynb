{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d005a5e3-a361-4e21-a0f2-2402bf05cdd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Data Preparation in ML - Notebook 02\n",
    "## Data Validation\n",
    "\n",
    "**Part of the Databricks Data Preparation in ML Training Series**\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook demonstrates essential data validation techniques for Databricks ML Associate Certification:\n",
    "\n",
    "- **Schema Validation** - Verifying data types and structure consistency\n",
    "- **Quality Metrics** - Measuring completeness, validity, and reliability\n",
    "- **Business Rules** - Implementing and validating business logic constraints\n",
    "- **Data Monitoring** - Setting up basic data quality monitoring workflows\n",
    "\n",
    "## Duration: ~30 minutes\n",
    "## Level: Intermediate\n",
    "\n",
    "---\n",
    "\n",
    "## Why is Data Validation Critical?\n",
    "\n",
    "**Data quality** forms the foundation of every successful ML project:\n",
    "- **Garbage In, Garbage Out** - Poor data quality leads to unreliable models\n",
    "- **Early Detection** - Identifying issues before they impact downstream processes\n",
    "- **Trust & Reliability** - Building confidence in data-driven decisions\n",
    "- **Cost Efficiency** - Preventing expensive fixes in production systems\n",
    "\n",
    "---\n",
    "\n",
    "## Data Quality Dimensions\n",
    "\n",
    "### Core Quality Metrics:\n",
    "\n",
    "#### **Completeness**\n",
    "- Are all required values present in the dataset?\n",
    "- What percentage of records have missing critical fields?\n",
    "\n",
    "#### **Validity** \n",
    "- Do values conform to expected formats and ranges?\n",
    "- Are categorical values from the expected domain?\n",
    "\n",
    "#### **Uniqueness**\n",
    "- Are there unwanted duplicate records?\n",
    "- Do business keys maintain their intended uniqueness?\n",
    "\n",
    "#### **Business Rules Compliance**\n",
    "- Do data relationships make business sense?\n",
    "- Are domain-specific constraints satisfied?\n",
    "\n",
    "### Validation in ML Context:\n",
    "- **Feature Quality** - Ensure features meet model expectations\n",
    "- **Training Data Integrity** - Validate consistency across training sets\n",
    "- **Inference Monitoring** - Detect data drift in production\n",
    "- **Pipeline Reliability** - Maintain data quality throughout ML workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61779a4b-a2d2-4c13-b913-29d8118109a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee5b1b0d-d456-418b-a81d-2df210159d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mUruchomienie kom√≥rek z elementem ‚ÄûPython 3.9.6‚Äù wymaga pakietu ipykernel.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Utw√≥rz ≈õrodowisko jƒôzyka Python</a> z wymaganymi pakietami.\n",
      "\u001b[1;31mLub zainstaluj ‚Äûipykernel‚Äù przy u≈ºyciu polecenia: ‚Äû/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall‚Äù"
     ]
    }
   ],
   "source": [
    "# Basic imports for Databricks ML\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, isnan, isnull, mean, stddev, min as spark_min, max as spark_max, percentile_approx, length\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6440104-f1dd-49bc-9afa-4885b7169564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b19245a-04e0-47c2-b6a3-2464f3e7b3b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Init\n",
    "fake = Faker()\n",
    "random.seed(42)\n",
    "Faker.seed(42)\n",
    "\n",
    "n_rows = 1000\n",
    "generated_names = []\n",
    "generated_emails = []\n",
    "generated_salaries = []\n",
    "sources = [\"CRM_SYSTEM\", \"HR_SYSTEM\", \"ERP_SYSTEM\", \"WEB_PORTAL\", \"MOBILE_APP\"]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i in range(1, n_rows + 1):\n",
    "    # --- full_name ---\n",
    "    name_choice = random.choices([\"new\", \"duplicate\", \"null\"], weights=[0.75, 0.2, 0.05])[0]\n",
    "    if name_choice == \"new\":\n",
    "        full_name = fake.name()\n",
    "        generated_names.append(full_name)\n",
    "    elif name_choice == \"duplicate\" and generated_names:\n",
    "        full_name = random.choice(generated_names)\n",
    "    else:\n",
    "        full_name = None\n",
    "\n",
    "    # --- age (as string) ---\n",
    "    age_choice = random.choices([\"valid\", \"invalid\", \"null\"], weights=[0.85, 0.1, 0.05])[0]\n",
    "    if age_choice == \"valid\":\n",
    "        age = str(random.randint(18, 65))\n",
    "    elif age_choice == \"invalid\":\n",
    "        age = \"xyz\"\n",
    "    else:\n",
    "        age = None\n",
    "\n",
    "    # --- email ---\n",
    "    email_choice = random.choices([\"new\", \"duplicate\", \"null\"], weights=[0.75, 0.2, 0.05])[0]\n",
    "    if email_choice == \"new\" and full_name:\n",
    "        email = f\"{full_name.replace(' ', '.').lower()}@example.com\"\n",
    "        generated_emails.append(email)\n",
    "    elif email_choice == \"duplicate\" and generated_emails:\n",
    "        email = random.choice(generated_emails)\n",
    "    else:\n",
    "        email = None\n",
    "\n",
    "    # --- salary (as string) ---\n",
    "    salary_choice = random.choices([\"new\", \"duplicate\", \"null\"], weights=[0.7, 0.25, 0.05])[0]\n",
    "    if salary_choice == \"new\":\n",
    "        salary = str(round(random.uniform(30000, 120000), 2))\n",
    "        generated_salaries.append(salary)\n",
    "    elif salary_choice == \"duplicate\" and generated_salaries:\n",
    "        salary = str(random.choice(generated_salaries))\n",
    "    else:\n",
    "        salary = None\n",
    "\n",
    "    # --- registration_date ---\n",
    "    registration_date = fake.date_between(start_date=\"-2y\", end_date=\"today\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # --- source_system ---\n",
    "    source_system = random.choice(sources)\n",
    "\n",
    "    rows.append((i, full_name, age, email, salary, registration_date, source_system))\n",
    "\n",
    "# --- Define schema ---\n",
    "bronze_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"full_name\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"salary\", StringType(), True),\n",
    "    StructField(\"registration_date\", StringType(), True),\n",
    "    StructField(\"source_system\", StringType(), True)\n",
    "])\n",
    "\n",
    "# --- Create DataFrame ---\n",
    "df_raw = spark.createDataFrame(rows, schema=bronze_schema)\n",
    "\n",
    "# --- Display ---\n",
    "display(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd835c5f-5403-4f02-95d2-7f064a206e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Schema Validation\n",
    "\n",
    "## Theory\n",
    "\n",
    "**Schema Validation** ensures that data conforms to the expected structure and format:\n",
    "- **Data types** - Verify columns have correct data types (integer, string, double, etc.)\n",
    "- **Nullable constraints** - Check if required fields are properly populated\n",
    "- **Column presence** - Ensure all expected columns exist in the dataset\n",
    "- **Column ordering** - Validate consistent column positioning for downstream processes\n",
    "\n",
    "### üéØ Schema Validation Benefits:\n",
    "- **Early Error Detection** - Catch structural issues before processing\n",
    "- **Pipeline Stability** - Prevent runtime errors in downstream applications\n",
    "- **Data Contract Enforcement** - Ensure producers meet consumer expectations\n",
    "- **ML Model Consistency** - Validate feature schema matches training expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3715a011-9546-489a-954f-9c7b89480f85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Basic schema validation\n",
    "expected_columns = [\"customer_id\", \"full_name\", \"age\", \"email\", \"salary\"]\n",
    "actual_columns = df_raw.columns\n",
    "\n",
    "display(df_raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dcfd8c0-623d-4ac1-a995-ebc6faabfa1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "missing_columns = set(expected_columns) - set(actual_columns)\n",
    "extra_columns = set(actual_columns) - set(expected_columns)\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"‚ùå Missing columns: {missing_columns}\")\n",
    "if extra_columns:\n",
    "    print(f\"‚ö†Ô∏è Extra columns: {extra_columns}\")\n",
    "if not missing_columns and not extra_columns:\n",
    "    print(\"‚úÖ All expected columns present\")\n",
    "\n",
    "# Check data types\n",
    "print(\"üìä Current data types:\")\n",
    "for col_name, col_type in df_raw.dtypes:\n",
    "    print(f\"   {col_name}: {col_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfcb7068-7614-4e8e-911b-54c989eb09a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Profiling\n",
    "\n",
    "*The cell below computes summary statistics (count, mean, stddev, min, max, etc.) for each numeric column in df_raw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "313c51ee-ec94-4020-b215-4a365df5c297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The cell below computes summary statistics (count, mean, stddev, min, max, etc.) for each numeric column in df_raw.\n",
    "display(df_raw.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec8b771-6ba4-4992-87f8-24f84c28bf77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85f7f980-6c9e-4e53-8f21-c85eed75a771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col# Check for null values in each column\n",
    "\n",
    "# Check for null or empty strings\n",
    "print(\"Empty string values or null value:\")\n",
    "string_columns = [col_name for col_name, col_type in df_raw.dtypes if col_type == 'string']\n",
    "for column in string_columns:\n",
    "    empty_count = df_raw.filter((col(column) == \"\") | (col(column).isNull())).count()\n",
    "    print(f\"   {column}: {empty_count} empty/null values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9adac179-d6e1-4f19-9af4-d791b0d40613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Type Validation\n",
    "\n",
    "This section explains how to verify that each column in your dataset has the expected data type. Data type validation ensures that numerical fields are not accidentally stored as strings, dates are properly formatted, and all columns match the schema required for downstream ML tasks. The code below will demonstrate how to check and enforce correct data types in your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "936ee716-d4fa-42ba-8e85-8ccdae1d51ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data type validation for each column\n",
    "expected_types = {\n",
    "    \"customer_id\": \"int\",\n",
    "    \"full_name\": \"string\",\n",
    "    \"age\": \"string\",\n",
    "    \"email\": \"string\",\n",
    "    \"salary\": \"int\",\n",
    "    \"registration_date\": \"string\",\n",
    "    \"source_system\": \"string\"\n",
    "}\n",
    "\n",
    "actual_types = dict(df_raw.dtypes)\n",
    "\n",
    "type_mismatches = []\n",
    "for col_name, expected_type in expected_types.items():\n",
    "    actual_type = actual_types.get(col_name)\n",
    "    if actual_type != expected_type:\n",
    "        type_mismatches.append((col_name, expected_type, actual_type))\n",
    "\n",
    "if type_mismatches:\n",
    "    print(\"‚ùå Data type mismatches found:\")\n",
    "    for col_name, expected, actual in type_mismatches:\n",
    "        print(f\"   {col_name}: expected {expected}, found {actual}\")\n",
    "else:\n",
    "    print(\"‚úÖ All column data types are correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5c18ca4-8e2c-4872-93ad-9ec3411dfc5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f62849bb-f2ee-4426-818f-c126a8ce06ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Data Quality Metrics\n",
    "\n",
    "## Theory\n",
    "\n",
    "**Data Quality Metrics** provide quantitative measures of data health across multiple dimensions. These metrics help establish baselines and monitor data quality over time.\n",
    "\n",
    "### üìä Key Quality Dimensions:\n",
    "\n",
    "#### **Completeness**\n",
    "- Percentage of non-null values in each column\n",
    "- Critical for identifying missing data patterns\n",
    "- Threshold: Typically >95% for critical fields\n",
    "\n",
    "#### **Uniqueness**\n",
    "- Percentage of unique values in key fields\n",
    "- Essential for primary keys and unique identifiers\n",
    "- Threshold: 100% for business keys\n",
    "\n",
    "#### **Validity**\n",
    "- Percentage of values conforming to expected format/range\n",
    "- Includes format validation (emails, phone numbers)\n",
    "- Range validation (age between 0-120, positive salaries)\n",
    "\n",
    "#### **Consistency**\n",
    "- Uniform representation across similar fields\n",
    "- Standardized formatting and naming conventions\n",
    "- Cross-field relationship validation\n",
    "\n",
    "### üéØ Quality Scoring:\n",
    "- **Excellent**: >95% - Production ready\n",
    "- **Good**: 85-95% - Minor issues, acceptable with monitoring\n",
    "- **Poor**: <85% - Requires immediate attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d95b2007-8f1b-4f43-9020-a821f8ee703b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This cell selects the `id` and `review_str` columns from the `example_table` for a specific date, and adds a new column `review_sentiment` that classifies the sentiment of each review as positive, negative, neutral, or mixed using the `ai_analyze_sentiment` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9248de3a-4df0-41c3-aaa8-4f8f9cb8384e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate basic quality metrics\n",
    "total_records = df_raw.count()\n",
    "\n",
    "print(f\"Data Quality Assessment: Total records: {total_records}\")\n",
    "print()\n",
    "\n",
    "# Completeness check for each column\n",
    "print(\"Completeness (% non-null values):\")\n",
    "for column in df_raw.columns:\n",
    "    null_count = df_raw.filter(col(column).isNull()).count()\n",
    "    completeness = (total_records - null_count) / total_records * 100\n",
    "    status = \"‚úÖ\" if completeness >= 95 else \"‚ö†Ô∏è\" if completeness >= 85 else \"‚ùå\"\n",
    "    print(f\"   {column}: {completeness:.1f}% {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4e832c9-ebd8-4739-97ee-97125c524fe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Completeness\n",
    "\n",
    "**Completeness** measures the extent to which all required data is present in a dataset. It focuses on identifying missing or null values in critical fields, ensuring that essential information is available for analysis and modeling. High completeness is crucial for reliable ML outcomes, as missing data can lead to biased models and inaccurate predictions. Typical completeness checks include calculating the percentage of non-null values per column and flagging records with missing mandatory fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9f30632-a6dd-44bf-b238-bab5717d1192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Uniqueness\n",
    "\n",
    "**Uniqueness** ensures that each record or key field in a dataset is distinct and not duplicated. This is critical for maintaining data integrity, especially for primary keys, unique identifiers, or business keys. Duplicate records can lead to inaccurate analytics, skewed model training, and operational errors. Typical uniqueness checks involve identifying duplicate rows or values in key columns and quantifying the percentage of unique entries. High uniqueness is essential for reliable ML outcomes and trustworthy data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77379d11-f1b7-48a3-9b31-b1c523d1544e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uniqueness check for ID columns\n",
    "print(\"üîç Uniqueness check:\")\n",
    "unique_ids = df_raw.select(\"customer_id\").distinct().count()\n",
    "total_ids = df_raw.filter(col(\"customer_id\").isNotNull()).count()\n",
    "uniqueness = unique_ids / total_ids * 100 if total_ids > 0 else 0\n",
    "\n",
    "print(f\"Customer ID uniqueness: {uniqueness:.1f}%\")\n",
    "if uniqueness < 100:\n",
    "    duplicate_count = total_ids - unique_ids\n",
    "    print(f\"   Found {duplicate_count} duplicate customer IDs\")\n",
    "    \n",
    "    # Show duplicate IDs\n",
    "    duplicates = df_raw.groupBy(\"customer_id\").count().filter(col(\"count\") > 1)\n",
    "    duplicates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e12ce08-1e19-4b28-bfbd-e620820541f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Validity - Business Rule Validation\n",
    "\n",
    "## Theory\n",
    "\n",
    "**Business Rules** are domain-specific constraints that data must satisfy to be considered valid for business use. These rules encode organizational knowledge and operational requirements.\n",
    "\n",
    "### üè¢ Types of Business Rules:\n",
    "\n",
    "#### **Range Constraints**\n",
    "- Age must be between 18-100 years for employee records\n",
    "- Salary must be positive and within reasonable bounds\n",
    "- Dates must be within expected business periods\n",
    "\n",
    "#### **Format Requirements**\n",
    "- Email addresses must follow standard email format\n",
    "- Phone numbers must match regional patterns\n",
    "- Postal codes must conform to country standards\n",
    "\n",
    "#### **Relationship Rules**\n",
    "- Start date must be before end date\n",
    "- Manager salary should be higher than direct reports\n",
    "- Department codes must exist in reference tables\n",
    "\n",
    "#### **Business Logic Constraints**\n",
    "- Minimum experience requirements for certain roles\n",
    "- Credit limits based on customer categories\n",
    "- Inventory levels must be non-negative\n",
    "\n",
    "### üéØ Implementation Strategy:\n",
    "- **Declarative Rules** - Define rules as SQL conditions\n",
    "- **Threshold-based** - Set acceptable compliance percentages\n",
    "- **Exception Handling** - Document and track rule violations\n",
    "- **Business Context** - Rules should reflect real business needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6698afa-d995-4cb2-b328-72c985e974c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Business Rule 1: Age must be between 18 and 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a44d9105-a9e8-4912-9abe-f457c4879ddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"üè¢ Business Rule Validation:\")\n",
    "print()\n",
    "\n",
    "# Age validation\n",
    "invalid_age = df_raw.filter(\n",
    "    col(\"age\").isNotNull() & \n",
    "    ((col(\"age\") < 18) | (col(\"age\") > 100))\n",
    ").count()\n",
    "\n",
    "age_compliance = (total_records - invalid_age) / total_records * 100\n",
    "print(f\"Age Range (18-100): {age_compliance:.1f}% compliant\")\n",
    "if invalid_age > 0:\n",
    "    print(f\"   Found {invalid_age} invalid ages\")\n",
    "    df_raw.filter((col(\"age\") < 18) | (col(\"age\") > 100)).select(\"customer_id\", \"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4210ffd-5be7-4972-a555-70c091987f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Business Rule 2: Salary must be positive and reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e36eee0-bbd3-4d0e-89fd-4070e42236d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Business Rule 2: Salary must be positive and reasonable\n",
    "invalid_salary = df_raw.filter(\n",
    "    col(\"salary\").isNotNull() & \n",
    "    ((col(\"salary\") <= 0) | (col(\"salary\") > 500000))\n",
    ").count()\n",
    "\n",
    "salary_compliance = (total_records - invalid_salary) / total_records * 100\n",
    "print(f\"Salary Range (>0, <500k): {salary_compliance:.1f}% compliant\")\n",
    "if invalid_salary > 0:\n",
    "    print(f\"   Found {invalid_salary} invalid salaries\")\n",
    "    df_raw.filter((col(\"salary\") <= 0) | (col(\"salary\") > 500000)).select(\"customer_id\", \"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f060de0a-a1b9-49b6-ac1f-661daee4477e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Business Rule 3: Email format validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0326a7e-c453-41c1-9106-b72a3fb0b5e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Business Rule 3: Email format validation\n",
    "email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "invalid_email = df_raw.filter(\n",
    "    col(\"email\").isNotNull() & \n",
    "    (~col(\"email\").rlike(email_pattern))\n",
    ").count()\n",
    "\n",
    "email_compliance = (total_records - invalid_email) / total_records * 100\n",
    "print(f\"Email Format: {email_compliance:.1f}% compliant\")\n",
    "if invalid_email > 0:\n",
    "    print(f\"   Found {invalid_email} invalid emails\")\n",
    "    df_raw.filter(col(\"email\").isNotNull() & (~col(\"email\").rlike(email_pattern))).select(\"customer_id\", \"email\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77eb53e9-ee0a-4c08-acfd-4c97229afe3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Statistical Validation\n",
    "\n",
    "## Theory\n",
    "\n",
    "**Statistical Validation** uses statistical methods to detect anomalies, outliers, and distribution changes that may indicate data quality issues or unexpected patterns.\n",
    "\n",
    "### üìä Statistical Validation Techniques:\n",
    "\n",
    "#### **Descriptive Statistics**\n",
    "- Mean, median, standard deviation for central tendency\n",
    "- Min/max values for range validation\n",
    "- Percentiles for distribution understanding\n",
    "\n",
    "#### **Outlier Detection**\n",
    "- **IQR Method** - Values beyond 1.5 * IQR from Q1/Q3\n",
    "- **Z-Score** - Values more than 3 standard deviations from mean\n",
    "- **Business Context** - Domain-specific outlier definitions\n",
    "\n",
    "#### **Distribution Analysis**\n",
    "- **Skewness** - Measure of asymmetry in data distribution\n",
    "- **Kurtosis** - Measure of tail heaviness\n",
    "- **Normality Tests** - Validate distributional assumptions\n",
    "\n",
    "#### **Temporal Validation**\n",
    "- **Trend Analysis** - Detect unusual patterns over time\n",
    "- **Seasonality** - Validate expected seasonal patterns\n",
    "- **Change Point Detection** - Identify significant distribution shifts\n",
    "\n",
    "### üéØ ML Applications:\n",
    "- **Feature Quality** - Ensure features have expected statistical properties\n",
    "- **Data Drift Detection** - Monitor changes in production data\n",
    "- **Anomaly Detection** - Identify unusual patterns for investigation\n",
    "- **Model Validation** - Verify training data statistical assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0355db2-9328-4f12-8b67-b1cb1e2b2d91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev, min as spark_min, max as spark_max, percentile_approx, col\n",
    "\n",
    "# Basic statistical validation for age\n",
    "\n",
    "age_stats_df = df_raw.filter(col(\"age\").isNotNull()).select(\n",
    "    mean(\"age\").alias(\"mean\"),\n",
    "    stddev(\"age\").alias(\"std\"),\n",
    "    spark_min(\"age\").alias(\"min_val\"),\n",
    "    spark_max(\"age\").alias(\"max_val\"),\n",
    "    percentile_approx(\"age\", 0.25).alias(\"q1\"),\n",
    "    percentile_approx(\"age\", 0.75).alias(\"q3\")\n",
    ")\n",
    "\n",
    "age_stats = age_stats_df.collect()[0]\n",
    "\n",
    "display(age_stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cff9e79d-7031-4b70-83ff-fd60b981f1eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Outlier Detection Using IQR Method (for age column)\n",
    "\n",
    "This section applies the Interquartile Range (IQR) method to detect outliers in the age column of the dataset.\n",
    "\n",
    " Step-by-step explanation:\n",
    "\n",
    "\t1.\tCompute IQR\n",
    "IQR is the range between the third quartile (Q3) and the first quartile (Q1):\n",
    "{IQR} = Q3 - Q1\n",
    "\n",
    "\t2.\tDefine bounds for outliers\n",
    "Any value that lies below the lower bound or above the upper bound is considered an outlier:\n",
    "{Lower Bound} = Q1 - 1.5 {IQR}\n",
    "{Upper Bound} = Q3 + 1.5 {IQR}\n",
    "\n",
    "\t3.\tFilter outliers in the age column\n",
    "\t‚Ä¢\tNull values are ignored\n",
    "\t‚Ä¢\tOnly values outside the [lower_bound, upper_bound] range are selected\n",
    "\t4.\tDisplay the results\n",
    "The filtered DataFrame contains only customers whose age is considered an outlier.\n",
    "\n",
    "Use case:\n",
    "\n",
    "This method is commonly used in data cleaning and feature engineering to identify values that may require further inspection, correction, or removal before training machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d481f6-7ea3-40b1-aa84-50ce8e982197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "iqr = age_stats['q3'] - age_stats['q1']\n",
    "lower_bound = age_stats['q1'] - 1.5 * iqr\n",
    "upper_bound = age_stats['q3'] + 1.5 * iqr\n",
    "\n",
    "outliers_df = df_raw.filter(\n",
    "    col(\"age\").isNotNull() & \n",
    "    ((col(\"age\") < lower_bound) | (col(\"age\") > upper_bound))\n",
    ")\n",
    "\n",
    "\n",
    "in_outliers_df = df_raw.filter(\n",
    "    col(\"age\").isNotNull() & \n",
    "    ((col(\"age\") >= lower_bound) | (col(\"age\") <= upper_bound))\n",
    ")\n",
    "\n",
    "display(outliers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de3cce13-e919-4de2-b25f-859c12b70786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw.filter(col(\"age\").isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a400468-b1b9-4d5f-b4d8-83412aae3085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c4a918-af20-4415-b929-7c9414c82610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "in_outliers_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c4bbb0f-2a2d-4358-91af-fbaf721efad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "outliers_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faa8e40b-fc34-4e9c-a4fd-82fbbfab174f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_raw = df_raw.withColumn(\"age\", col(\"age\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d313da2e-22b3-45e8-bc5b-66cab9228cf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Comprehensive Data Quality Report\n",
    "\n",
    "## Production-Ready Data Quality Scorecard\n",
    "\n",
    "Create a comprehensive data quality assessment that can be used for:\n",
    "- **Executive Reporting** - High-level data health summary\n",
    "- **Operational Monitoring** - Daily/weekly quality tracking\n",
    "- **ML Pipeline Validation** - Pre-training data quality checks\n",
    "- **Compliance Documentation** - Audit trail for data governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "188bba3f-6e83-4116-8c8c-d137c4ea1664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simple Data Quality Report\n",
    "total_records = df_raw.count()\n",
    "\n",
    "# 1. Completeness check\n",
    "print(\"üìä COMPLETENESS:\")\n",
    "completeness_scores = []\n",
    "for column in df_raw.columns:\n",
    "    null_count = df_raw.filter(col(column).isNull()).count()\n",
    "    completeness = (total_records - null_count) / total_records * 100\n",
    "    completeness_scores.append(completeness)\n",
    "    status = \"‚úÖ\" if completeness >= 95 else \"‚ö†Ô∏è\" if completeness >= 85 else \"‚ùå\"\n",
    "    print(f\"   {column}: {completeness:.1f}% {status}\")\n",
    "\n",
    "avg_completeness = sum(completeness_scores) / len(completeness_scores)\n",
    "\n",
    "# 2. Business rules compliance\n",
    "print(\"\\nüè¢ BUSINESS RULES:\")\n",
    "age_violations = df_raw.filter((col(\"age\") < 18) | (col(\"age\") > 100)).count()\n",
    "age_compliance = (total_records - age_violations) / total_records * 100\n",
    "print(f\"   Age (18-100): {age_compliance:.1f}% {'‚úÖ' if age_compliance >= 90 else '‚ùå'}\")\n",
    "\n",
    "salary_violations = df_raw.filter((col(\"salary\") <= 0) | (col(\"salary\") > 500000)).count()\n",
    "salary_compliance = (total_records - salary_violations) / total_records * 100\n",
    "print(f\"   Salary (>0, <500k): {salary_compliance:.1f}% {'‚úÖ' if salary_compliance >= 90 else '‚ùå'}\")\n",
    "\n",
    "avg_compliance = (age_compliance + salary_compliance) / 2\n",
    "\n",
    "# 3. Overall score\n",
    "overall_score = (avg_completeness * 0.6 + avg_compliance * 0.4)\n",
    "\n",
    "if overall_score >= 90:\n",
    "    grade = \"A (Excellent)\"\n",
    "elif overall_score >= 80:\n",
    "    grade = \"B (Good)\"\n",
    "elif overall_score >= 70:\n",
    "    grade = \"C (Fair)\"\n",
    "else:\n",
    "    grade = \"D (Poor)\"\n",
    "\n",
    "print(f\"\\nüéØ OVERALL QUALITY SCORE: {overall_score:.1f}% - {grade}\")\n",
    "\n",
    "# Simple recommendations\n",
    "print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "if overall_score < 90:\n",
    "    print(\"   ‚Ä¢ Review and improve data quality before ML training\")\n",
    "    print(\"   ‚Ä¢ Fix business rule violations\")\n",
    "    print(\"   ‚Ä¢ Handle missing values appropriately\")\n",
    "else:\n",
    "    print(\"   ‚Ä¢ Data quality is good - proceed with ML pipeline\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e0ae38b-3c4b-4c76-8424-02608ac31bd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## QUICK REFERENCE for Databricks ML Associate\n",
    "\n",
    "### **1Ô∏è‚É£ Check for Null Values:**\n",
    "```python\n",
    "null_count = df.filter(col('column').isNull()).count()\n",
    "completeness = (total - null_count) / total * 100\n",
    "```\n",
    "\n",
    "### **2Ô∏è‚É£ Business Rule Validation:**\n",
    "```python\n",
    "violations = df.filter((col('age') < 18) | (col('age') > 100)).count()\n",
    "compliance = (total - violations) / total * 100\n",
    "```\n",
    "\n",
    "### **3Ô∏è‚É£ Email Format Validation:**\n",
    "```python\n",
    "pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "valid_emails = df.filter(col('email').rlike(pattern))\n",
    "```\n",
    "\n",
    "### **4Ô∏è‚É£ Schema Validation:**\n",
    "```python\n",
    "expected_columns = [\"id\", \"name\", \"email\"]\n",
    "actual_columns = df.columns\n",
    "missing = set(expected_columns) - set(actual_columns)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You have completed **Data Validation in Databricks ML**!  \n",
    "\n",
    "### Key Skills Acquired:\n",
    "- ‚úÖ Schema validation techniques\n",
    "- ‚úÖ Data quality metrics calculation  \n",
    "- ‚úÖ Business rule implementation\n",
    "- ‚úÖ Simple monitoring frameworks"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8406406073236294,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Data_Validation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
